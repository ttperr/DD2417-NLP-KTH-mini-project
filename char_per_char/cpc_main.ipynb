{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "202e5b05-663a-4be5-adaf-088482568f94",
   "metadata": {},
   "source": [
    "# Question word prediction\n",
    "\n",
    "> Group 12: Tristan Perrot & Romain Darous\n",
    "\n",
    "Task is to train and evaluate a **char per char Transformer model** model using any available QA-corpus, for instance, the [SQuAD corpus](https://rajpurkar.github.io/SQuAD-explorer/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ff4b64",
   "metadata": {},
   "source": [
    "METTRE EN CONTEXTE LE DEBUT DE LA QUESTION EN CORRIGEANT LE CODE DEJA FAIT\n",
    "ADAPTER POUR AUGMENTER LE NOMBRE DE TRANSFORMERS EVENTUELLEMENT ET FAIRE PLUSIEURS COUCHES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36f3a0c",
   "metadata": {},
   "source": [
    "# 0. Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37285237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "\n",
    "# Importing\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import random\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# Importing models\n",
    "import char_dataset\n",
    "import cpc_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "359b715e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(torch.cuda.device_count()):\n",
    "    print(torch.cuda.get_device_properties(i).name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available(\n",
    ") else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c009446",
   "metadata": {},
   "source": [
    "# 1. Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50678183",
   "metadata": {},
   "source": [
    "## 1.1. Loading the dataset\n",
    "**Note :** we only want to be able te recover the beginning of a question. For that, it doesn\"t matter whether the question is impossible to answer or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a59dd1d-d079-4907-95a6-f7a24b8aea24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions: 86821\n",
      "\n",
      "Question: When did Beyonce start becoming popular?\n",
      "Answer: \n",
      "\n",
      "Question: What areas did Beyonce compete in when she was growing up?\n",
      "Answer: \n",
      "\n",
      "Question: When did Beyonce leave Destiny's Child and become a solo singer?\n",
      "Answer: \n",
      "\n",
      "Question: In what city and state did Beyonce  grow up? \n",
      "Answer: \n",
      "\n",
      "Question: In which decade did Beyonce become famous?\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "data_dir = 'data'\n",
    "if data_dir not in os.listdir():\n",
    "    os.mkdir(data_dir)\n",
    "\n",
    "if \"squad_train.json\" not in os.listdir(data_dir):\n",
    "    # Download data at https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
    "    res = requests.get(\n",
    "        \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\")\n",
    "    data = json.loads(res.text)\n",
    "\n",
    "    # Save data to file\n",
    "    with open(data_dir + \"/squad_train.json\", \"w\") as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "with open(data_dir + \"/squad_train.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract answer text and question text\n",
    "answers = []\n",
    "questions = []\n",
    "for article in data[\"data\"]:\n",
    "    for paragraph in article[\"paragraphs\"]:\n",
    "        for qa in paragraph[\"qas\"]:\n",
    "            \n",
    "            \"\"\"if qa[\"is_impossible\"] :\n",
    "                continue\n",
    "            answers.append(qa[\"answers\"][0][\"text\"])\"\"\"\n",
    "                \n",
    "            if qa[\"is_impossible\"] and len(qa[\"question\"]) > 4:\n",
    "                continue\n",
    "            else :\n",
    "                answers.append(\"\")\n",
    "            questions.append(qa[\"question\"])\n",
    "            \n",
    "\n",
    "print(\"Number of questions:\", len(questions))\n",
    "\n",
    "# Print some examples\n",
    "for i in range(5):\n",
    "    print()\n",
    "    print(\"Question:\", questions[i])\n",
    "    print(\"Answer:\", answers[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d185c76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In which decade did Beyonce become famous? \n"
     ]
    }
   ],
   "source": [
    "print(questions[i] + ' ' + answers[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1ddcfb",
   "metadata": {},
   "source": [
    "## 1.2. Making a suitable dataset\n",
    "``<BOS>`` token. Indicates that the sentence is starting.\n",
    "\n",
    "We will make the prediction of the sentence in reverse mode, as we want to predict the beginning of a question. We will use unidirectionnal attention as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2805192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating questions and answers\n",
    "dataset = [(questions[i].lower() + ' ' + answers[i].lower())[::-1] for i in range(len(questions))]\n",
    "\n",
    "# Shuffle dataset\n",
    "random.shuffle(dataset)\n",
    "\n",
    "# Splitting into train, validation, and test sets\n",
    "train_dataset, test_dataset = train_test_split(dataset, test_size = int(0.1*len(dataset)), train_size=int(0.9*len(dataset)))\n",
    "train_dataset, val_dataset = train_test_split(train_dataset, train_size=int(0.85*len(train_dataset)), test_size = int(0.15*len(train_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1fcf626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dataset : 86821\n",
      "Size of the train, val and test sets : (66417, 11720, 8682)\n",
      "Example of original datapoint : When did Beyonce start becoming popular? \n",
      "Example of formatted datapoint :  ?nosib tnuh yeht did revir hcihw gnola\n"
     ]
    }
   ],
   "source": [
    "print(f\"Size of the dataset : {len(dataset)}\")\n",
    "print(f\"Size of the train, val and test sets : {len(train_dataset), len(val_dataset), len(test_dataset)}\")\n",
    "print(f\"Example of original datapoint : {questions[0] + ' ' + answers[0]}\")\n",
    "print(f\"Example of formatted datapoint : {dataset[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f52619",
   "metadata": {},
   "source": [
    "## 1.3. Building a character dataset\n",
    "The dataset will be built in the ``train_charlm`` function as it depends on the desired configuration. The function that allow to build the dataset and their context are provided in the file ``char_dataset.py``."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1c2b1c",
   "metadata": {},
   "source": [
    "# 2. The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f91fb4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating models# Delete the modules from the namespace\n",
    "del char_dataset\n",
    "del cpc_model\n",
    "\n",
    "# Unload the modules from memory\n",
    "import sys\n",
    "del sys.modules['char_dataset']\n",
    "del sys.modules['cpc_model']\n",
    "\n",
    "# Importing models\n",
    "import char_dataset\n",
    "import cpc_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853e5b06",
   "metadata": {},
   "source": [
    "## 2.0. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8317878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= Hyper-parameter class for training ============== #\n",
    "\n",
    "class Config :\n",
    "    def __init__(self, number_of_transformer_encoders = 4, number_of_attention_heads = 4, hidden_size = 256, \n",
    "             dropout_prob = 0.1, batch_size = 64, learning_rate = 0.0003, weight_decay = 0.000001, no_of_epochs = 100,\n",
    "             is_causal = True, seq_ctxt = True, MAXLEN = 32) :\n",
    "        \n",
    "        self.number_of_transformer_encoders = 4\n",
    "        self.number_of_attention_heads = 4\n",
    "        self.hidden_size = 256\n",
    "        self.dropout_prob = 0.1\n",
    "        self.batch_size = 5\n",
    "        self.learning_rate = 0.0003\n",
    "        self.weight_decay = 0.000001\n",
    "        self.no_of_epochs = 2\n",
    "        self.is_causal = True # When True, the attention is causal\n",
    "        self.seq_ctxt = True # When False, forces the context to take the beginning of the answer into account\n",
    "        self.MAXLEN = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e7de04",
   "metadata": {},
   "source": [
    "## 2.1. Training\n",
    "Defining functions for training and testing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab7b047d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_charlm(config) :\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    # ==================== Building datasets ================ #\n",
    "    train_char_set = char_dataset.CharDataset(train_dataset, config.MAXLEN, seq_ctxt=config.seq_ctxt)\n",
    "    val_char_set = char_dataset.CharDataset(val_dataset, config.MAXLEN, seq_ctxt=config.seq_ctxt)\n",
    "\n",
    "    # ======================= Training ======================= #\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print( \"Running on\", device )\n",
    "\n",
    "    training_loader = DataLoader(train_char_set, batch_size=config.batch_size)\n",
    "    validation_loader = DataLoader(val_char_set, batch_size=config.batch_size)\n",
    "\n",
    "    charlm = cpc_model.CharLM( config, len(char_dataset.CharDataset.id_to_char), config.MAXLEN, config.is_causal).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    charlm_optimizer = optim.Adam( charlm.parameters(), lr=config.learning_rate )\n",
    "\n",
    "\n",
    "    val_loss = 0.0\n",
    "    patience = 3\n",
    "    best_val_loss = torch.inf\n",
    "\n",
    "    charlm.train()\n",
    "    print( datetime.now().strftime(\"%X\"), \"Training starts\" )\n",
    "\n",
    "    for epoch in tqdm(range(config.no_of_epochs)) :\n",
    "        iteration = 0\n",
    "        for input_tensor, label in training_loader :\n",
    "            input_tensor, label = input_tensor.to(device), label.to(device)\n",
    "            charlm_optimizer.zero_grad()\n",
    "            logits = charlm(input_tensor).to(device)\n",
    "            loss = criterion(logits.squeeze(1), label)\n",
    "            loss.backward()\n",
    "            charlm_optimizer.step()\n",
    "            iteration += 1\n",
    "\n",
    "        print( datetime.now().strftime(\"%X\"), \"End of epoch\", epoch+1, \", loss=\", loss.detach().item())\n",
    "        \n",
    "        # Validation phase with Early Stopping\n",
    "        charlm.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for input_tensor, label in validation_loader:\n",
    "                input_tensor, label = input_tensor.to(device), label.to(device)\n",
    "                logits = charlm(input_tensor).to(device)\n",
    "                loss = criterion(logits.squeeze(1), label)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(validation_loader)\n",
    "        print(datetime.now().strftime(\"%X\"), \"Validation loss=\", val_loss)\n",
    "\n",
    "        # Check early stopping condition\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience = 5\n",
    "        else:\n",
    "            patience -= 1\n",
    "\n",
    "        if patience == 0:\n",
    "            print(\"Early stopping at epoch\", epoch + 1)\n",
    "            break\n",
    "\n",
    "        charlm.train()\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "\n",
    "    return charlm, epoch + 1, end_time - start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a55bc1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_charlm(charlm, config) :\n",
    "    # Computing output on the test set\n",
    "    test_char_set = char_dataset.CharDataset(test_dataset, config.MAXLEN, seq_ctxt=config.seq_ctxt)\n",
    "    test_loader = DataLoader(test_char_set, batch_size=len(test_char_set))\n",
    "\n",
    "    charlm.eval()\n",
    "    with torch.no_grad():\n",
    "        for input_tensor, label in test_loader:\n",
    "            input_tensor, label = input_tensor.to(device), label.to(device)\n",
    "            logits = charlm(input_tensor).to(device)\n",
    "            pred_label = torch.argmax(logits, axis = -1)[:,0]\n",
    "            accuracy += (pred_label == label).sum() / len(pred_label)\n",
    "\n",
    "    print(f\"Test accuracy : {torch.round(accuracy*100, decimals = 2)} %\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e535d7c7",
   "metadata": {},
   "source": [
    "## 2.2. Grid search on parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a13ff8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "Running on cpu\n",
      "10:01:03 Training starts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62adffcef99b41b795e3d0428970ec36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(config\u001b[38;5;241m.\u001b[39mdropout_prob)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Training the model\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m model, epoch, delta \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_charlm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m test_charlm(model, config)\n\u001b[0;32m     14\u001b[0m models\u001b[38;5;241m.\u001b[39mappend(models)\n",
      "Cell \u001b[1;32mIn[10], line 36\u001b[0m, in \u001b[0;36mtrain_charlm\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m     34\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(logits\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m), label)\n\u001b[0;32m     35\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 36\u001b[0m     \u001b[43mcharlm_optimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     iteration \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m( datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%X\u001b[39;00m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnd of epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m, epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, loss=\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\Users\\romai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    389\u001b[0m             )\n\u001b[1;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\romai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\romai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\adam.py:168\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    157\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    159\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    160\u001b[0m         group,\n\u001b[0;32m    161\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    165\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    166\u001b[0m         state_steps)\n\u001b[1;32m--> 168\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\romai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\adam.py:318\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 318\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\romai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\adam.py:393\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    390\u001b[0m     param \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(param)\n\u001b[0;32m    392\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m--> 393\u001b[0m \u001b[43mexp_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlerp_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    394\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "output = './output/'\n",
    "model_names = ['charlm_seq_32_es', 'charlm_seq_64_es', 'charlm_nseq_32_es', 'charlm_nseq_64_es']\n",
    "configs = [Config(seq_ctxt=True, MAXLEN=32), Config(seq_ctxt=True, MAXLEN=64),Config(seq_ctxt=False, MAXLEN=32), Config(seq_ctxt=False, MAXLEN=64)]\n",
    "test_acc = []\n",
    "models = []\n",
    "train_time = []\n",
    "epochs = []\n",
    "\n",
    "for i, config in enumerate(configs) :\n",
    "    # Training the model\n",
    "    model, epoch, delta = train_charlm(config)\n",
    "    accuracy = test_charlm(model, config)\n",
    "    models.append(models)\n",
    "    test_acc.append(accuracy)\n",
    "    train_time.append(delta)\n",
    "    epochs.append(epoch)\n",
    "    \n",
    "\n",
    "    # Saving the model\n",
    "    torch.save(model.state_dict(), f\"{output}{model_names[i]}{epoch}\")\n",
    "    print(\"Model saved successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea2ff53",
   "metadata": {},
   "source": [
    "Building a ``.csv`` file to store the results :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a94f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'Running time' : train_time,\n",
    "    'Last epoch' : epochs,\n",
    "    'Context size' : [cfg.MAXLEN for cfg in configs],\n",
    "    'Type of context' : ['Sequential' if cfg.seq_ctxt else 'Split' for cfg in configs],\n",
    "    'Test accuracy (%)' : test_acc\n",
    "}\n",
    "\n",
    "# Create the DataFrame with row names\n",
    "df = pd.DataFrame(data, index=model_names)\n",
    "df.to_csv(output + 'metrics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c74b7d9",
   "metadata": {},
   "source": [
    "## 2.2. Loading a model\n",
    "Now that we trained several models, we can load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff5b22e-0ba7-4242-ab4e-ae203fb89ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Load a model\n",
    "charlm = cpc_model.CharLM( config, len(char_dataset.CharDataset.id_to_char), config.MAXLEN, config.is_causal).to(device)\n",
    "charlm.load_state_dict(torch.load('./output/charlm_seq_model_early_stopping.pth'))\n",
    "charlm.eval()\"\"\"\n",
    "\n",
    "# Getting the best model\n",
    "best_model_idx = np.argmax(test_acc)\n",
    "charlm = models[best_model_idx]\n",
    "config = configs[best_model_idx]\n",
    "print(\"Best model :\", df.loc[model_names[best_model_idx]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d51919",
   "metadata": {},
   "source": [
    "## 2.2. Evaluation on the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09448cd7",
   "metadata": {},
   "source": [
    "## 2.3. User interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9511c57",
   "metadata": {},
   "source": [
    "New context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c85141b",
   "metadata": {},
   "source": [
    "Original context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b64382f-0ae7-493f-b4f5-6a9294c9b500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== User interaction ==================== #\n",
    "while True:\n",
    "    text = input(\"> \").strip()\n",
    "    if text == \"\" :\n",
    "        continue\n",
    "    elif text == \"QUIT\" :\n",
    "        break\n",
    "\n",
    "    # Will be used to output question\n",
    "    full_question = list(text.lower())[::-1]\n",
    "\n",
    "    if char_list[-1] != ' ' : \n",
    "        char_list.append(' ')\n",
    "        char_list = char_list[1:]\n",
    "        full_question.append(' ')\n",
    "        \n",
    "    new_character = char_list[-1]\n",
    "\n",
    "    # Recovering the beginning of the question\n",
    "    try :\n",
    "        count = 0\n",
    "        MAX_COUNT = 50\n",
    "        while new_character != char_dataset.CharDataset.BOQ and count < MAX_COUNT :\n",
    "            # Building context\n",
    "            char_list = []\n",
    "            if config.seq_ctxt :\n",
    "                char_list = full_question[-config.MAXLEN:]\n",
    "            else :\n",
    "                if len(full_question.split(\"?\")) == 1 : \n",
    "                    words_a = full_question.split(\"?\")[0]\n",
    "                    words_q = \"\"\n",
    "                else : words_a, words_q = full_question.split(\"?\")[0], full_question.split(\"?\")[1]\n",
    "                len_q = len(words_q) # counting the <BOS> token\n",
    "                if len_q <= config.MAXLEN//2 or len(words_a) < config.MAXLEN//8 :\n",
    "                    char_list = full_question[-config.MAXLEN:]\n",
    "                else :\n",
    "                    char_list = words_a[config.MAXLEN//2:] + full_question[-config.MAXLEN:]\n",
    "            \n",
    "            ctxt = [char_dataset.CharDataset.char_to_id[c] for c in char_list]\n",
    "            \n",
    "            \n",
    "            # Computing the next character\n",
    "            input_tensor = torch.tensor( [0]*(config.MAXLEN-len(ctxt)) + ctxt).unsqueeze(0).to(device)\n",
    "            logits = charlm(input_tensor).squeeze().to(device)\n",
    "            _, new_character_tensor = logits.topk(1)\n",
    "            new_character = char_dataset.CharDataset.id_to_char[new_character_tensor.detach().item()]\n",
    "            \n",
    "            # Uploading the final output\n",
    "            full_question.append(new_character)\n",
    "            count += 1\n",
    "\n",
    "        full_question = \"\".join(full_question[::-1])\n",
    "        print(f\"Recovered question : {full_question}\")\n",
    "    except KeyError :\n",
    "        print(\"ERROR\")\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
