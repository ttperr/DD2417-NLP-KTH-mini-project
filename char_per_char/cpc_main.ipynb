{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "202e5b05-663a-4be5-adaf-088482568f94",
   "metadata": {},
   "source": [
    "# Question word prediction\n",
    "\n",
    "> Group 12: Tristan Perrot & Romain Darous\n",
    "\n",
    "Task is to train and evaluate a **char per char Transformer model** model using any available QA-corpus, for instance, the [SQuAD corpus](https://rajpurkar.github.io/SQuAD-explorer/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ff4b64",
   "metadata": {},
   "source": [
    "METTRE EN CONTEXTE LE DEBUT DE LA QUESTION EN CORRIGEANT LE CODE DEJA FAIT\n",
    "ADAPTER POUR AUGMENTER LE NOMBRE DE TRANSFORMERS EVENTUELLEMENT ET FAIRE PLUSIEURS COUCHES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36f3a0c",
   "metadata": {},
   "source": [
    "# 0. Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37285237",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "\n",
    "# Importing\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import random\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# Importing models\n",
    "import char_dataset\n",
    "import cpc_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c009446",
   "metadata": {},
   "source": [
    "# 1. Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50678183",
   "metadata": {},
   "source": [
    "## 1.1. Loading the dataset\n",
    "**Note :** we only want to be able te recover the beginning of a question. For that, it doesn\"t matter whether the question is impossible to answer or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a59dd1d-d079-4907-95a6-f7a24b8aea24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions: 130319\n",
      "\n",
      "Question: When did Beyonce start becoming popular?\n",
      "Answer: in the late 1990s\n",
      "\n",
      "Question: What areas did Beyonce compete in when she was growing up?\n",
      "Answer: singing and dancing\n",
      "\n",
      "Question: When did Beyonce leave Destiny's Child and become a solo singer?\n",
      "Answer: 2003\n",
      "\n",
      "Question: In what city and state did Beyonce  grow up? \n",
      "Answer: Houston, Texas\n",
      "\n",
      "Question: In which decade did Beyonce become famous?\n",
      "Answer: late 1990s\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'data'\n",
    "if data_dir not in os.listdir():\n",
    "    os.mkdir(data_dir)\n",
    "\n",
    "if \"squad_train.json\" not in os.listdir(data_dir):\n",
    "    # Download data at https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
    "    res = requests.get(\n",
    "        \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\")\n",
    "    data = json.loads(res.text)\n",
    "\n",
    "    # Save data to file\n",
    "    with open(data_dir + \"/squad_train.json\", \"w\") as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "with open(data_dir + \"/squad_train.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract answer text and question text\n",
    "answers = []\n",
    "questions = []\n",
    "for article in data[\"data\"]:\n",
    "    for paragraph in article[\"paragraphs\"]:\n",
    "        for qa in paragraph[\"qas\"]:             \n",
    "            if qa[\"is_impossible\"] and len(qa[\"question\"]) > 4:\n",
    "                answers.append(\"\")\n",
    "            else :\n",
    "                answers.append(qa[\"answers\"][0]['text'])\n",
    "            questions.append(qa[\"question\"])\n",
    "            \n",
    "\n",
    "print(\"Number of questions:\", len(questions))\n",
    "\n",
    "# Print some examples\n",
    "for i in range(5):\n",
    "    print()\n",
    "    print(\"Question:\", questions[i])\n",
    "    print(\"Answer:\", answers[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d185c76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In which decade did Beyonce become famous? late 1990s\n"
     ]
    }
   ],
   "source": [
    "print(questions[i] + ' ' + answers[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1ddcfb",
   "metadata": {},
   "source": [
    "## 1.2. Making a suitable dataset\n",
    "``<BOS>`` token. Indicates that the sentence is starting.\n",
    "\n",
    "We will make the prediction of the sentence in reverse mode, as we want to predict the beginning of a question. We will use unidirectionnal attention as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2805192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating questions and answers\n",
    "dataset = [(questions[i].lower() + ' ' + answers[i].lower())[::-1] for i in range(len(questions))]\n",
    "\n",
    "# Shuffle dataset\n",
    "random.shuffle(dataset)\n",
    "\n",
    "# Splitting into train, validation, and test sets\n",
    "train_dataset, test_dataset = train_test_split(dataset, test_size = int(0.1*len(dataset)), train_size=int(0.9*len(dataset)))\n",
    "train_dataset, val_dataset = train_test_split(train_dataset, train_size=int(0.85*len(train_dataset)), test_size = int(0.15*len(train_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1fcf626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dataset : 130319\n",
      "Size of the train, val and test sets : (99693, 17593, 13031)\n",
      "Example of original datapoint : When did Beyonce start becoming popular? in the late 1990s\n",
      "Example of formatted datapoint :  ?sevitavresnoc eht fo citsiretcarahc gninifed a si tahw\n"
     ]
    }
   ],
   "source": [
    "print(f\"Size of the dataset : {len(dataset)}\")\n",
    "print(f\"Size of the train, val and test sets : {len(train_dataset), len(val_dataset), len(test_dataset)}\")\n",
    "print(f\"Example of original datapoint : {questions[0] + ' ' + answers[0]}\")\n",
    "print(f\"Example of formatted datapoint : {dataset[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f52619",
   "metadata": {},
   "source": [
    "## 1.3. Building a character dataset\n",
    "The dataset will be built in the ``train_charlm`` function as it depends on the desired configuration. The function that allow to build the dataset and their context are provided in the file ``char_dataset.py``."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1c2b1c",
   "metadata": {},
   "source": [
    "# 2. The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f91fb4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<PAD>': 0, '<BOQ>': 1, '<UNK>': 2, 'a': 3, 'b': 4, 'c': 5, 'd': 6, 'e': 7, 'f': 8, 'g': 9, 'h': 10, 'i': 11, 'j': 12, 'k': 13, 'l': 14, 'm': 15, 'n': 16, 'o': 17, 'p': 18, 'q': 19, 'r': 20, 's': 21, 't': 22, 'u': 23, 'v': 24, 'w': 25, 'x': 26, 'y': 27, 'z': 28, ' ': 29, ',': 30, '.': 31, '-': 32, ';': 33, ':': 34, '!': 35, '?': 36, '0': 37, '1': 38, '2': 39, '3': 40, '4': 41, '5': 42, '6': 43, '7': 44, '8': 45, '9': 46}\n",
      "{'<PAD>': 0, '<BOQ>': 1, '<UNK>': 2, 'a': 3, 'b': 4, 'c': 5, 'd': 6, 'e': 7, 'f': 8, 'g': 9, 'h': 10, 'i': 11, 'j': 12, 'k': 13, 'l': 14, 'm': 15, 'n': 16, 'o': 17, 'p': 18, 'q': 19, 'r': 20, 's': 21, 't': 22, 'u': 23, 'v': 24, 'w': 25, 'x': 26, 'y': 27, 'z': 28, ' ': 29, ',': 30, '.': 31, '-': 32, ';': 33, ':': 34, '!': 35, '?': 36, '0': 37, '1': 38, '2': 39, '3': 40, '4': 41, '5': 42, '6': 43, '7': 44, '8': 45, '9': 46}\n"
     ]
    }
   ],
   "source": [
    "# Updating models# Delete the modules from the namespace\n",
    "del char_dataset\n",
    "del cpc_model\n",
    "\n",
    "# Unload the modules from memory\n",
    "import sys\n",
    "del sys.modules['char_dataset']\n",
    "del sys.modules['cpc_model']\n",
    "\n",
    "# Importing models\n",
    "import char_dataset\n",
    "import cpc_model\n",
    "\n",
    "# Initalizing the vocabulary\n",
    "char_dataset.CharDataset(\"init\", 0)\n",
    "print(char_dataset.CharDataset.char_to_id)\n",
    "print(char_dataset.CharDataset.char_to_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853e5b06",
   "metadata": {},
   "source": [
    "## 2.0. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8317878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= Hyper-parameter class for training ============== #\n",
    "\n",
    "class Config :\n",
    "    def __init__(self, seq_ctxt = True, MAXLEN = 32) : \n",
    "        self.number_of_transformer_encoders = 2\n",
    "        self.number_of_attention_heads = 2\n",
    "        self.hidden_size = 64\n",
    "        self.dropout_prob = 0.1\n",
    "        self.batch_size = 64\n",
    "        self.learning_rate = 0.0003\n",
    "        self.weight_decay = 0.000001\n",
    "        self.no_of_epochs = 100\n",
    "        self.is_causal = True # When True, the attention is causal\n",
    "        self.seq_ctxt = seq_ctxt # When False, forces the context to take the beginning of the answer into account\n",
    "        self.MAXLEN = MAXLEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e7de04",
   "metadata": {},
   "source": [
    "## 2.1. Training\n",
    "Defining functions for training and testing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab7b047d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_charlm(config, device) :\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    # ==================== Building datasets ================ #\n",
    "    train_char_set = char_dataset.CharDataset(train_dataset, config.MAXLEN, \n",
    "                                              seq_ctxt=config.seq_ctxt)\n",
    "    val_char_set = char_dataset.CharDataset(val_dataset, config.MAXLEN, \n",
    "                                            seq_ctxt=config.seq_ctxt)\n",
    "\n",
    "    # ======================= Training ======================= #\n",
    "\n",
    "    \n",
    "\n",
    "    training_loader = DataLoader(train_char_set, batch_size=config.batch_size)\n",
    "    validation_loader = DataLoader(val_char_set, batch_size=config.batch_size)\n",
    "\n",
    "    charlm = cpc_model.CharLM( config, len(char_dataset.CharDataset.id_to_char), \n",
    "                              config.MAXLEN, config.is_causal).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    charlm_optimizer = optim.Adam( charlm.parameters(), lr=config.learning_rate )\n",
    "\n",
    "\n",
    "    patience = 5\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    charlm.train()\n",
    "    print( datetime.now().strftime(\"%X\"), \"Training starts\" )\n",
    "\n",
    "    iteration = 0\n",
    "    for epoch in tqdm(range(config.no_of_epochs)) :\n",
    "        val_loss = 0.0\n",
    "        for input_tensor, label in training_loader :\n",
    "            input_tensor, label = input_tensor.to(device), label.to(device)\n",
    "            charlm_optimizer.zero_grad()\n",
    "            logits = charlm(input_tensor).to(device)\n",
    "            loss = criterion(logits.squeeze(1), label)\n",
    "            loss.backward()\n",
    "            charlm_optimizer.step()\n",
    "        iteration += 1\n",
    "\n",
    "        print( datetime.now().strftime(\"%X\"), \"End of epoch\", epoch+1, \", loss=\", loss.detach().item())\n",
    "        \n",
    "        # Validation phase with Early Stopping\n",
    "        charlm.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for input_tensor, label in validation_loader:\n",
    "                input_tensor, label = input_tensor.to(device), label.to(device)\n",
    "                logits = charlm(input_tensor).to(device)\n",
    "                loss = criterion(logits.squeeze(1), label)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(validation_loader)\n",
    "\n",
    "        # Check early stopping condition\n",
    "        if val_loss <= best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience = 5\n",
    "        else:\n",
    "            patience -= 1\n",
    "            if patience == 0:\n",
    "                print(\"Early stopping at epoch\", epoch + 1)\n",
    "                break\n",
    "        print(datetime.now().strftime(\"%X\"), \"Validation loss=\", val_loss, \"Patience=\", patience)\n",
    "\n",
    "        charlm.train()\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "\n",
    "    return charlm, iteration + 1, end_time - start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a55bc1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_charlm(model, config, device) :\n",
    "\n",
    "    # Computing output on the test set\n",
    "    test_char_set = char_dataset.CharDataset(test_dataset, config.MAXLEN, seq_ctxt=config.seq_ctxt)\n",
    "    test_loader = DataLoader(test_char_set, batch_size=config.batch_size)\n",
    "\n",
    "    accuracies = []\n",
    "    model.to(device)\n",
    "    model.eval()    \n",
    "    with torch.no_grad():\n",
    "        for input_tensor, label in test_loader:\n",
    "            input_tensor, label = input_tensor.to(device), label.to(device)\n",
    "            logits = model(input_tensor).to(device)\n",
    "            _, pred_label = logits.topk(1, dim=-1)\n",
    "            pred_label = pred_label.squeeze(-1)  # Squeeze to remove extra dimension\n",
    "            accuracy = (pred_label == label).sum()/len(test_loader)  # Compute accuracy for this batch\n",
    "            accuracies.append(accuracy.item())  # Append batch accuracy to list)\n",
    "    total_acc = np.round(np.mean(np.array(accuracies)*100), 2)\n",
    "    print(f\"Test accuracy : {total_acc} %\")\n",
    "    return total_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e535d7c7",
   "metadata": {},
   "source": [
    "## 2.2. Grid search on parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f35d7e83-fa4e-4145-ad8e-adb1e2f180e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print( \"Running on\", device )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13ff8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05:45:38 Training starts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c28035969b4f159c53bdb59466385d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05:46:33 End of epoch 1 , loss= 0.167288139462471\n",
      "05:46:37 Validation loss= 1.0499531774975808 Patience= 5\n",
      "05:47:32 End of epoch 2 , loss= 0.15651434659957886\n",
      "05:47:36 Validation loss= 0.9790533195536044 Patience= 5\n",
      "05:48:30 End of epoch 3 , loss= 0.19782622158527374\n",
      "05:48:34 Validation loss= 0.9477244002764301 Patience= 5\n",
      "05:49:28 End of epoch 4 , loss= 0.13825955986976624\n",
      "05:49:32 Validation loss= 0.9231590684226882 Patience= 5\n",
      "05:50:26 End of epoch 5 , loss= 0.16033703088760376\n",
      "05:50:30 Validation loss= 0.9068655153256002 Patience= 5\n",
      "05:51:24 End of epoch 6 , loss= 0.14073769748210907\n",
      "05:51:28 Validation loss= 0.895369708201068 Patience= 5\n",
      "05:52:22 End of epoch 7 , loss= 0.13588538765907288\n",
      "05:52:26 Validation loss= 0.8896045930920558 Patience= 5\n",
      "05:53:20 End of epoch 8 , loss= 0.13501837849617004\n",
      "05:53:24 Validation loss= 0.8811629133384556 Patience= 5\n",
      "05:54:19 End of epoch 9 , loss= 0.09646333009004593\n",
      "05:54:23 Validation loss= 0.8742896638889616 Patience= 5\n",
      "05:55:17 End of epoch 10 , loss= 0.14080986380577087\n",
      "05:55:21 Validation loss= 0.8711926472271289 Patience= 5\n",
      "05:56:14 End of epoch 11 , loss= 0.1699114292860031\n",
      "05:56:18 Validation loss= 0.8645527264991835 Patience= 5\n",
      "05:57:12 End of epoch 12 , loss= 0.1197512298822403\n",
      "05:57:16 Validation loss= 0.8596150213648489 Patience= 5\n",
      "05:58:09 End of epoch 13 , loss= 0.22077512741088867\n",
      "05:58:13 Validation loss= 0.8562052949473209 Patience= 5\n",
      "05:59:07 End of epoch 14 , loss= 0.17418651282787323\n",
      "05:59:11 Validation loss= 0.8546412165299743 Patience= 5\n",
      "06:00:05 End of epoch 15 , loss= 0.1840635985136032\n",
      "06:00:08 Validation loss= 0.850875958937217 Patience= 5\n",
      "06:01:03 End of epoch 16 , loss= 0.13249583542346954\n",
      "06:01:06 Validation loss= 0.8473834473758198 Patience= 5\n",
      "06:02:00 End of epoch 17 , loss= 0.20421500504016876\n",
      "06:02:04 Validation loss= 0.8462325732088763 Patience= 5\n",
      "06:02:58 End of epoch 18 , loss= 0.2552773356437683\n",
      "06:03:02 Validation loss= 0.844214685045367 Patience= 5\n",
      "06:03:56 End of epoch 19 , loss= 0.19423189759254456\n",
      "06:04:00 Validation loss= 0.8447793336618074 Patience= 4\n",
      "06:04:54 End of epoch 20 , loss= 0.188723623752594\n",
      "06:04:58 Validation loss= 0.8398572938812916 Patience= 5\n",
      "06:05:52 End of epoch 21 , loss= 0.2519398629665375\n",
      "06:05:56 Validation loss= 0.8371028874977738 Patience= 5\n",
      "06:06:50 End of epoch 22 , loss= 0.15039688348770142\n",
      "06:06:54 Validation loss= 0.836512050885615 Patience= 5\n",
      "06:07:48 End of epoch 23 , loss= 0.17383721470832825\n",
      "06:07:52 Validation loss= 0.8374297616127945 Patience= 4\n",
      "06:08:47 End of epoch 24 , loss= 0.2071978896856308\n",
      "06:08:50 Validation loss= 0.8367354788333704 Patience= 3\n",
      "06:09:45 End of epoch 25 , loss= 0.1543472558259964\n",
      "06:09:49 Validation loss= 0.8346184384591167 Patience= 5\n",
      "06:10:43 End of epoch 26 , loss= 0.16143973171710968\n",
      "06:10:47 Validation loss= 0.831269991450512 Patience= 5\n",
      "06:11:42 End of epoch 27 , loss= 0.11168304830789566\n",
      "06:11:46 Validation loss= 0.828621321347914 Patience= 5\n",
      "06:12:40 End of epoch 28 , loss= 0.17432408034801483\n",
      "06:12:44 Validation loss= 0.83026082570477 Patience= 4\n",
      "06:13:38 End of epoch 29 , loss= 0.08121232688426971\n",
      "06:13:42 Validation loss= 0.8272166519093429 Patience= 5\n",
      "06:14:37 End of epoch 30 , loss= 0.20004622638225555\n",
      "06:14:41 Validation loss= 0.8281992434612854 Patience= 4\n",
      "06:15:36 End of epoch 31 , loss= 0.15564990043640137\n",
      "06:15:40 Validation loss= 0.8256582669887441 Patience= 5\n",
      "06:16:35 End of epoch 32 , loss= 0.1908312886953354\n",
      "06:16:39 Validation loss= 0.8250104210086088 Patience= 5\n",
      "06:17:33 End of epoch 33 , loss= 0.19159474968910217\n",
      "06:17:37 Validation loss= 0.8239681042230592 Patience= 5\n",
      "06:18:32 End of epoch 34 , loss= 0.10918661952018738\n",
      "06:18:36 Validation loss= 0.8249983287952815 Patience= 4\n",
      "06:19:33 End of epoch 35 , loss= 0.22368718683719635\n",
      "06:19:36 Validation loss= 0.8226579276706641 Patience= 5\n",
      "06:20:32 End of epoch 36 , loss= 0.19571806490421295\n",
      "06:20:36 Validation loss= 0.8228859088762067 Patience= 4\n",
      "06:21:31 End of epoch 37 , loss= 0.11734408885240555\n",
      "06:21:34 Validation loss= 0.8232364481729669 Patience= 3\n",
      "06:22:29 End of epoch 38 , loss= 0.20651909708976746\n",
      "06:22:33 Validation loss= 0.8210873596870436 Patience= 5\n",
      "06:23:27 End of epoch 39 , loss= 0.15839725732803345\n",
      "06:23:31 Validation loss= 0.819033722626868 Patience= 5\n",
      "06:24:25 End of epoch 40 , loss= 0.29130586981773376\n",
      "06:24:29 Validation loss= 0.8210461368409147 Patience= 4\n",
      "06:25:24 End of epoch 41 , loss= 0.20379386842250824\n",
      "06:25:27 Validation loss= 0.8174594400207061 Patience= 5\n",
      "06:26:21 End of epoch 42 , loss= 0.2913414537906647\n",
      "06:26:25 Validation loss= 0.8176276172850242 Patience= 4\n",
      "06:27:19 End of epoch 43 , loss= 0.2101014405488968\n",
      "06:27:23 Validation loss= 0.8165456878106923 Patience= 5\n",
      "06:28:17 End of epoch 44 , loss= 0.16155757009983063\n",
      "06:28:21 Validation loss= 0.8155331444403308 Patience= 5\n",
      "06:29:16 End of epoch 45 , loss= 0.1218947023153305\n",
      "06:29:20 Validation loss= 0.8151619226477592 Patience= 5\n",
      "06:30:15 End of epoch 46 , loss= 0.250592440366745\n",
      "06:30:19 Validation loss= 0.816856509657715 Patience= 4\n",
      "06:31:14 End of epoch 47 , loss= 0.10378143191337585\n",
      "06:31:17 Validation loss= 0.8172973411032673 Patience= 3\n",
      "06:32:13 End of epoch 48 , loss= 0.18675674498081207\n",
      "06:32:17 Validation loss= 0.8148040287275617 Patience= 5\n",
      "06:33:12 End of epoch 49 , loss= 0.09412292391061783\n",
      "06:33:16 Validation loss= 0.8138512704587235 Patience= 5\n",
      "06:34:10 End of epoch 50 , loss= 0.22904306650161743\n",
      "06:34:14 Validation loss= 0.811486529356178 Patience= 5\n",
      "06:35:10 End of epoch 51 , loss= 0.1447429209947586\n",
      "06:35:13 Validation loss= 0.8136614078451804 Patience= 4\n",
      "06:36:11 End of epoch 52 , loss= 0.22593684494495392\n",
      "06:36:14 Validation loss= 0.8131368906043023 Patience= 3\n",
      "06:37:11 End of epoch 53 , loss= 0.296591579914093\n",
      "06:37:15 Validation loss= 0.8128424937649245 Patience= 2\n",
      "06:38:10 End of epoch 54 , loss= 0.26790231466293335\n",
      "06:38:14 Validation loss= 0.8119808267788836 Patience= 1\n",
      "06:39:08 End of epoch 55 , loss= 0.17994430661201477\n",
      "06:39:12 Validation loss= 0.8108838271652431 Patience= 5\n",
      "06:40:07 End of epoch 56 , loss= 0.21282899379730225\n",
      "06:40:11 Validation loss= 0.808679575096592 Patience= 5\n",
      "06:41:06 End of epoch 57 , loss= 0.1489562839269638\n",
      "06:41:09 Validation loss= 0.8106949593806014 Patience= 4\n",
      "06:42:04 End of epoch 58 , loss= 0.23923009634017944\n",
      "06:42:08 Validation loss= 0.8078424101583528 Patience= 5\n",
      "06:43:02 End of epoch 59 , loss= 0.24411949515342712\n",
      "06:43:06 Validation loss= 0.8074516836724939 Patience= 5\n",
      "06:44:00 End of epoch 60 , loss= 0.17775076627731323\n",
      "06:44:04 Validation loss= 0.8082719560646758 Patience= 4\n",
      "06:44:59 End of epoch 61 , loss= 0.17689232528209686\n",
      "06:45:03 Validation loss= 0.8081844895235641 Patience= 3\n",
      "06:45:57 End of epoch 62 , loss= 0.10461309552192688\n",
      "06:46:01 Validation loss= 0.8073420597154766 Patience= 5\n",
      "06:46:55 End of epoch 63 , loss= 0.35506677627563477\n",
      "06:46:59 Validation loss= 0.8073633529272181 Patience= 4\n",
      "06:47:53 End of epoch 64 , loss= 0.2383863627910614\n",
      "06:47:57 Validation loss= 0.8069451783655389 Patience= 5\n",
      "06:48:51 End of epoch 65 , loss= 0.19233247637748718\n",
      "06:48:55 Validation loss= 0.8066140521225575 Patience= 5\n",
      "06:49:51 End of epoch 66 , loss= 0.14425048232078552\n",
      "06:49:54 Validation loss= 0.8066531515900743 Patience= 4\n",
      "06:50:51 End of epoch 67 , loss= 0.199623703956604\n",
      "06:50:54 Validation loss= 0.8056758205903293 Patience= 5\n"
     ]
    }
   ],
   "source": [
    "output = './output/'\n",
    "model_names = ['charlm_seq_32_es', 'charlm_seq_64_es', \n",
    "               'charlm_nseq_32_es', 'charlm_nseq_64_es']\n",
    "configs = [Config(seq_ctxt=True, MAXLEN=32), Config(seq_ctxt=True, MAXLEN=64),\n",
    "           Config(seq_ctxt=False, MAXLEN=32), Config(seq_ctxt=False, MAXLEN=64)]\n",
    "test_acc = []\n",
    "models = []\n",
    "train_time = []\n",
    "epochs = []\n",
    "\n",
    "for config, model_name in zip(configs, model_names) :\n",
    "    # Training the model\n",
    "    model, epoch, delta = train_charlm(config, device)\n",
    "    models.append(model)\n",
    "    train_time.append(delta)\n",
    "    epochs.append(epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c233ef4-023e-406a-8a8a-375e0f5ae8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, config, model_name in zip(models, configs, model_names) :\n",
    "    # Test accuracy\n",
    "    accuracy = test_charlm(model, config, device)\n",
    "    test_acc.append(accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4835e2-6c78-403e-9661-dd11c479e7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, model_name, epoch in zip(models, model_names, epochs) :\n",
    "    # Saving the model\n",
    "    torch.save(model.state_dict(), f\"{output}{model_name}{epoch}.pth\")\n",
    "    print(\"Model saved successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea2ff53",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Building a ``.csv`` file to store the results :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a94f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'Running time' : train_time,\n",
    "    'Last epoch' : epochs,\n",
    "    'Type of context' : ['Sequential' if cfg.seq_ctxt else 'Split' for cfg in configs],\n",
    "    'Context size' : [cfg.MAXLEN for cfg in configs],\n",
    "    'Test accuracy (%)' : test_acc\n",
    "}\n",
    "\n",
    "# Create the DataFrame with row names\n",
    "df = pd.DataFrame(data, index=model_names)\n",
    "df.to_csv(output + 'metrics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c74b7d9",
   "metadata": {},
   "source": [
    "## 2.2. Loading a model\n",
    "Now that we trained several models, we can load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff5b22e-0ba7-4242-ab4e-ae203fb89ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Load a model\n",
    "charlm = cpc_model.CharLM( config, len(char_dataset.CharDataset.id_to_char), config.MAXLEN, config.is_causal).to(device)\n",
    "charlm.load_state_dict(torch.load('./output/charlm_seq_model_early_stopping.pth'))\n",
    "charlm.eval()\"\"\"\n",
    "\n",
    "# Getting the best model\n",
    "best_model_idx = np.argmax(test_acc)\n",
    "charlm = models[best_model_idx]\n",
    "config = configs[best_model_idx]\n",
    "print(f\"Best model : {model_names[best_model_idx]} : \\n\\n\", df.loc[model_names[best_model_idx]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d51919",
   "metadata": {},
   "source": [
    "## 2.2. Evaluation on the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09448cd7",
   "metadata": {},
   "source": [
    "## 2.3. User interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b64382f-0ae7-493f-b4f5-6a9294c9b500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== User interaction ==================== #\n",
    "while True:\n",
    "    text = input(\"> \").strip()\n",
    "    if text == \"\" :\n",
    "        continue\n",
    "    elif text == \"QUIT\" :\n",
    "        break\n",
    "\n",
    "    # Will be used to output question\n",
    "    full_question = list(text.lower())[::-1]\n",
    "\n",
    "    if full_question[-1] != ' ' : \n",
    "        full_question.append(' ')\n",
    "    if '?' not in full_question :\n",
    "        full_question = ['?'] + full_question\n",
    "        \n",
    "    new_character = full_question[-1]\n",
    "\n",
    "    # Recovering the beginning of the question\n",
    "    try :\n",
    "        count = 0\n",
    "        MAX_COUNT = 50\n",
    "        while new_character != char_dataset.CharDataset.BOQ and count < MAX_COUNT :\n",
    "            # Building context\n",
    "            char_list = []\n",
    "            if config.seq_ctxt :\n",
    "                char_list = full_question[-config.MAXLEN:]\n",
    "            else :\n",
    "                tmp = \"\".join(full_question)\n",
    "                words_a, words_q = tmp.split(\"?\")[0], tmp.split(\"?\")[1]\n",
    "                words_a = list(words_a)\n",
    "                len_q = len(words_q) # counting the <BOS> token\n",
    "                if len_q <= config.MAXLEN//2 or len(words_a) < config.MAXLEN//8 :\n",
    "                    char_list = full_question[-config.MAXLEN:]\n",
    "                else :\n",
    "                    char_list = words_a[-config.MAXLEN//2:] + full_question[-config.MAXLEN//2:]\n",
    "\n",
    "            ctxt = [char_dataset.CharDataset.char_to_id[c] if c in char_dataset.CharDataset.char_to_id \n",
    "                    else char_dataset.CharDataset.char_to_id[char_dataset.CharDataset.char_to_id[char_dataset.CharDataset.UNK]] \n",
    "                    for c in char_list]            \n",
    "            \n",
    "            # Computing the next character\n",
    "            input_tensor = torch.tensor( [0]*(config.MAXLEN-len(ctxt)) + ctxt).unsqueeze(0).to(device)\n",
    "            logits = charlm(input_tensor).squeeze().to(device)\n",
    "            _, new_character_tensor = logits.topk(1)\n",
    "            id = new_character_tensor.detach().item()\n",
    "            new_character = char_dataset.CharDataset.id_to_char[id]\n",
    "            \n",
    "            # Uploading the final output\n",
    "            full_question.append(new_character)\n",
    "            count += 1\n",
    "\n",
    "        full_question = \"\".join(full_question[::-1])\n",
    "        print(f\"Recovered question : {full_question}\")\n",
    "    except KeyError :\n",
    "        print(\"ERROR\")\n",
    "        raise KeyError\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
