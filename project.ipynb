{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question word prediction\n",
    "\n",
    "> Group 12: Tristan Perrot & Romain Darous\n",
    "\n",
    "Task is to train and evaluate a QWP model using any available QA-corpus, for instance, the [SQuAD corpus](https://rajpurkar.github.io/SQuAD-explorer/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(torch.cuda.device_count()):\n",
    "    print(torch.cuda.get_device_properties(i).name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available(\n",
    ") else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions: 86821\n",
      "\n",
      "Question: When did Beyonce start becoming popular?\n",
      "Answer: in the late 1990s\n",
      "\n",
      "Question: What areas did Beyonce compete in when she was growing up?\n",
      "Answer: singing and dancing\n",
      "\n",
      "Question: When did Beyonce leave Destiny's Child and become a solo singer?\n",
      "Answer: 2003\n",
      "\n",
      "Question: In what city and state did Beyonce  grow up? \n",
      "Answer: Houston, Texas\n",
      "\n",
      "Question: In which decade did Beyonce become famous?\n",
      "Answer: late 1990s\n"
     ]
    }
   ],
   "source": [
    "if data_dir not in os.listdir():\n",
    "    os.mkdir(data_dir)\n",
    "\n",
    "if \"squad_train.json\" not in os.listdir(data_dir):\n",
    "    # Download data at https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
    "    res = requests.get(\n",
    "        \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\")\n",
    "    data = json.loads(res.text)\n",
    "\n",
    "    # Save data to file\n",
    "    with open(data_dir + \"/squad_train.json\", \"w\") as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "with open(data_dir + \"/squad_train.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract answer text and question text\n",
    "answers = []\n",
    "questions = []\n",
    "for article in data[\"data\"]:\n",
    "    for paragraph in article[\"paragraphs\"]:\n",
    "        for qa in paragraph[\"qas\"]:\n",
    "            if qa[\"is_impossible\"]:\n",
    "                continue\n",
    "            answers.append(qa[\"answers\"][0][\"text\"])\n",
    "            questions.append(qa[\"question\"])\n",
    "\n",
    "print(\"Number of questions:\", len(questions))\n",
    "\n",
    "# Print some examples\n",
    "for i in range(5):\n",
    "    print()\n",
    "    print(\"Question:\", questions[i])\n",
    "    print(\"Answer:\", answers[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: ['When', 'did', 'Beyonce', 'start', 'becoming', 'popular', '?']\n",
      "Answer: ['in', 'the', 'late', '1990s']\n",
      "\n",
      "Question: ['What', 'areas', 'did', 'Beyonce', 'compete', 'in', 'when', 'she', 'was', 'growing', 'up', '?']\n",
      "Answer: ['singing', 'and', 'dancing']\n",
      "\n",
      "Question: ['When', 'did', 'Beyonce', 'leave', 'Destiny', \"'s\", 'Child', 'and', 'become', 'a', 'solo', 'singer', '?']\n",
      "Answer: ['2003']\n",
      "\n",
      "Question: ['In', 'what', 'city', 'and', 'state', 'did', 'Beyonce', 'grow', 'up', '?']\n",
      "Answer: ['Houston', ',', 'Texas']\n",
      "\n",
      "Question: ['In', 'which', 'decade', 'did', 'Beyonce', 'become', 'famous', '?']\n",
      "Answer: ['late', '1990s']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize questions\n",
    "tokenized_questions = [nltk.word_tokenize(q) for q in questions]\n",
    "\n",
    "# Tokenize answers\n",
    "tokenized_answers = [nltk.word_tokenize(a) for a in answers]\n",
    "\n",
    "for i in range(5):\n",
    "    print()\n",
    "    print(\"Question:\", tokenized_questions[i])\n",
    "    print(\"Answer:\", tokenized_answers[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merged: ['When', 'did', 'Beyonce', 'start', 'becoming', 'popular', '?', 'in', 'the', 'late', '1990s']\n",
      "\n",
      "Merged: ['What', 'areas', 'did', 'Beyonce', 'compete', 'in', 'when', 'she', 'was', 'growing', 'up', '?', 'singing', 'and', 'dancing']\n",
      "\n",
      "Merged: ['When', 'did', 'Beyonce', 'leave', 'Destiny', \"'s\", 'Child', 'and', 'become', 'a', 'solo', 'singer', '?', '2003']\n",
      "\n",
      "Merged: ['In', 'what', 'city', 'and', 'state', 'did', 'Beyonce', 'grow', 'up', '?', 'Houston', ',', 'Texas']\n",
      "\n",
      "Merged: ['In', 'which', 'decade', 'did', 'Beyonce', 'become', 'famous', '?', 'late', '1990s']\n"
     ]
    }
   ],
   "source": [
    "# Merge questions and answers\n",
    "merged = [q + a for q, a in zip(tokenized_questions, tokenized_answers)]\n",
    "\n",
    "for i in range(5):\n",
    "    print()\n",
    "    print(\"Merged:\", merged[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary\n",
    "vocab = set()\n",
    "for m in merged:\n",
    "    vocab.update(m)\n",
    "\n",
    "vocab = list(vocab)\n",
    "\n",
    "# Add \"<qw>\" to vocabulary\n",
    "vocab.append(\"<qw>\")\n",
    "vocab.append(\"<pad>\")\n",
    "\n",
    "# Create word to index and index to word mappings\n",
    "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "idx_to_word = {i: word for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merged: ['When', 'did', 'Beyonce', 'start', 'becoming', 'popular', '?', 'in', 'the', 'late', '1990s']\n",
      "Hidden: ['<qw>', 'did', 'Beyonce', 'start', 'becoming', 'popular', '?', 'in', 'the', 'late', '1990s']\n",
      "\n",
      "Merged: ['What', 'areas', 'did', 'Beyonce', 'compete', 'in', 'when', 'she', 'was', 'growing', 'up', '?', 'singing', 'and', 'dancing']\n",
      "Hidden: ['<qw>', 'areas', 'did', 'Beyonce', 'compete', 'in', 'when', 'she', 'was', 'growing', 'up', '?', 'singing', 'and', 'dancing']\n",
      "\n",
      "Merged: ['When', 'did', 'Beyonce', 'leave', 'Destiny', \"'s\", 'Child', 'and', 'become', 'a', 'solo', 'singer', '?', '2003']\n",
      "Hidden: ['<qw>', 'did', 'Beyonce', 'leave', 'Destiny', \"'s\", 'Child', 'and', 'become', 'a', 'solo', 'singer', '?', '2003']\n",
      "\n",
      "Merged: ['In', 'what', 'city', 'and', 'state', 'did', 'Beyonce', 'grow', 'up', '?', 'Houston', ',', 'Texas']\n",
      "Hidden: ['<qw>', 'what', 'city', 'and', 'state', 'did', 'Beyonce', 'grow', 'up', '?', 'Houston', ',', 'Texas']\n",
      "\n",
      "Merged: ['In', 'which', 'decade', 'did', 'Beyonce', 'become', 'famous', '?', 'late', '1990s']\n",
      "Hidden: ['<qw>', 'which', 'decade', 'did', 'Beyonce', 'become', 'famous', '?', 'late', '1990s']\n",
      "\n",
      "{'Campaign', 'Whites', 'Besides', 'Programming', 'Hayek', 'Tags', 'Spoleto', 'Serious', 'IndyMac', '[', '11th', 'Whom', 'Broadcast', 'Innovations', 'Swaps', 'Midwesterners', 'So', 'Baltica', 'Ontological', 'Antagonistic', 'FighterCommand', 'Prior', 'Attempts', 'Cardinals', 'Single', 'Cicadas', 'Gaddafi', 'Edvard', 'Vienna', 'Materials', 'during', 'Tranquilizers', 'Thousands', 'Carolingian', 'Gandhi', 'Min', 'Compression', 'Municipalities', 'Avfuel', 'Approximately', 'Tower', 'Statistically', 'Defected', 'Combining', 'Prominent', 'Giovani', 'Evelyn', 'Valens', 'Beyer', 'Music', 'Weekly', 'Compensation', 'who', 'Excess', 'Rocket', 'Edmund', 'Toward', 'Former', 'PIccolo', 'After', 'Well-off', 'Libraries', 'PAL', 'Approval', 'Imperial', 'Kurent', 'Nader', 'Soyuz', 'Service', 'Coupling', 'Avoidance', 'LaserDisc', 'Symptoms', 'Well-fed', 'Earthquakes', 'Stepinac', 'Reservations', 'Capitalism', \"d'Alembert\", 'Malians', 'Antidepressants', 'Lord', 'Norfolk', 'Byte', 'Insects', 'Tourists', 'Education', 'Sputnik', 'Neurons', 'Longhorn', 'Fine', 'How', 'Musical', 'Schwarzenegger', 'Vertebrates', 'HDTV', 'Jewelry', 'Rosemary', 'Fachhochschulen', 'Korolev', 'Clementa', 'From', 'Josef', 'Denmark', 'Receptacles', 'Timing', 'Nigeria', 'Anglo', 'Preparing', 'She', 'Cubism', 'China', 'Archival', 'Increases', 'Adding', 'Foreign', 'THe', 'Nasser', 'Policy', 'Beginning', 'Aspiration', 'Protestantism', 'Strepsiptera', 'Familiarty', 'Socrate', 'Guatama', 'Deeply', 'Confrontational', 'Hinduism', 'Declining', 'Examinations', 'Season', 'Zen', 'Ring-porous', 'there', 'Geologically', 'Coordinative', 'Aspirated', 'Rummel', 'Sugar', 'Bismarck', 'An', 'Cities', 'Buridan', 'Public', 'Sunderland', 'Multiprocessor', 'Radio', 'Dominic', 'Godly', 'National', 'Rove', 'Exclusive', 'Devotion', 'Warfare', 'Drugs', 'Prey', 'Radical', 'Reflector', 'Active', 'Authors', 'what', 'Samuel', 'Freemasons', 'Firewalls', 'Tanasi', 'Texts', 'Google', 'Vehicle', 'RAM', 'James', 'Various', 'Idol', 'Than', 'Streptococci', 'A', 'Alaska', 'Complaints', 'Rabbi', 'Women', 'Hard', 'Opus', 'Muslims', 'Social', '3MBS', 'Football', 'Insulin', 'FLNG', 'RAF', 'Bad', 'when', 'Apellon', 'Sufficient', 'Ai-Khanoum', 'His', 'Neighborhoods', '120th', 'Alsace', 'Garuda', '13th', 'Nicky', 'Nixon', 'Europeans', 'Dog', 'Equipment', 'Prisoners', 'Duke', 'Link', 'Eugen', 'vytautas', 'Liberia', 'Angara', 'Similar', 'Bath', 'Surface-mount', 'Mariel', 'Laurel', 'condensed', 'Coat', 'Girls', 'Ehat', 'Cinnabar', 'Data', 'Ctenophora', 'Judges', 'Sentient', 'Noble', 'Hyundai', 'Gladstone', 'Veterans', 'Written', 'Explain', 'Sandakan', 'Measles', 'Microfilms', 'Affirmative', 'george', 'Miller', 'Lackluster', 'WHo', 'Society', 'Housing', 'Oxford', 'Frontiersmen', 'Assuming', 'iPods', 'Telegu', 'Flies', 'Rome', 'Sleeping', 'Convicted', 'Calculating', 'Selling', 'Sheryl', 'Insectivorous', 'Mimicry', 'MundoFox', 'Chopins', 'Zhejiang', 'Leaders', 'Pb2+', 'Baena', 'Paperback', 'Architecture', 'Konrad', 'Not', 'Often', 'Peking', 'Gas', 'Section', 'Voter', 'Players', 'Vine', 'Pat', 'Historian', 'Located', 'Structure', 'CPUs', 'Opera', 'Isoniazid', 'Sebastian', 'Traveling', 'Route', 'Italian', 'Beatport', 'PFOA', 'Towards', 'Köppen', 'Grapes', 'Middle', 'Had', 'Pig', 'Mass', 'Perpetrators', 'I2a1b1', 'France', 'Write', 'Galschiøt', 'Sammy', 'Engineered', 'Skilled', 'Layla', 'Minuscule', 'List', 'Overvaluation', 'Diabetes', 'Original', 'Evidence', 'Samatha', 'Stimulating', 'Hyderabadi', 'Chemical', 'Forbes.com', 'Catalan', 'Circa', 'Judea', 'Morphology-based', 'Ash', 'Mixed', 'Out', 'Maverick', 'German', 'Twilight', 'Colors', 'Discussion', 'Teletype', 'Islam', 'Physical', 'International', 'Hound', 'Lee', 'Her', 'Both', 'Whether', 'Tenuis', 'Venus', 'Heart', 'Following', 'Southern', 'Voltage', 'Hostile', 'ISO', 'Exactly', 'NATO', 'Gain', 'Arizona', 'Slash', 'Buddhist', 'Jesus', 'Alternatively', 'Casavantes', 'These', 'Hoe', 'percentage', 'Whas', 'Dragonflys', '22', 'Passenger', 'Scout', 'Oliver', 'Popular', 'Insect', 'Catholic', 'Jealous', 'Other', 'Turks', 'Hunt', 'Critical', 'America', 'Long', 'Growth', 'Smart', 'Further', 'Non-traditional', 'Knighthood', 'Trophy', 'Posting', 'You', 'Four-fifths', 'Leotards', 'Cleopatra', 'Impressive', 'Arterial', 'Segments', 'London', 'Artaxian', 'Polymophism', 'Futron', 'Morningside', 'Staring', 'A440', 'Pilot', 'Lingayen', 'Eventually', 'Humans', 'Vaccination', 'Constructs', 'Chopin', 'waht', 'Aphrodite', 'Rivals', 'Bharatpur', 'Raeder', 'Meditation', 'Muhammad', 'Vacuum', 'Pottery', 'Typical', 'Fuel', 'Least', 'Ranunculus', 'Comiskey', 'Directive', 'Included', 'Planning', 'Clinical', 'Conversely', 'Game', 'Bicycling', 'Millions', 'Animals', 'Molecules', 'Attendees', 'Ranthamb', 'Comparing', 'Members', 'Secure', 'U.S.', 'Kimat', 'Daniel', 'By', 'Blockbuster', 'Drug-resistant', 'Canada', 'Ag3Cu', 'Beliefs', 'Orthodox', 'Drinking', 'Interest', 'CJFC', 'Inside', 'Wat', 'Consumers', 'Omniprestent', 'Techno', 'Locke', 'Film', 'Gautama', 'Which', 'Proponents', 'de', 'December', 'Hydropower', 'Hydraulic', 'Pinochet', 'Finite', 'Booting', 'Alloys', 'Damage', 'Multitasking', 'Hormones', 'Shorter', 'Compared', 'Shiraz', 'Landside', 'East', 'Interstate', 'Videoconferencing', 'Chris', 'Atlantic', 'Fpr', 'Do', 'Calabar', 'Truman', 'Element', 'Kathmandu', 'Doubly', 'Mitochondrial', 'Regional', 'Web', 'Instructions', 'Toxocariasis', 'Cells', 'Gagan', 'Formal', 'utrecht', 'Rebirth', 'Japan', 'Turkish', 'Super', 'Contributions', 'Lots', 'Mid-18th', 'Hammer', 'Mali', 'Hight', 'US', 'Autosomal', 'Terrorist', 'Parish', 'Bell', 'Producers', 'Electric', 'Slovene', 'Bombing', 'Zhongshan', 'De', 'Cases', 'Attempting', 'Oppidan', 'Collembola', 'Analysts', 'Slavs', 'Action', 'Youtube', 'Relativity', 'Melbourne', 'Elisha', 'youtube', 'Christós', 'which', 'R-7', 'Arthur', 'Unequal', 'Menander', 'Advances', 'Major', 'Judaism', 'Ottoman', 'Quick', 'Keyboard', 'Neighbours', 'Intraguild', 'Pamlico', 'Dr', 'Alhazen', 'Notre', 'Bases', 'Deductions', 'Installing', 'Investigations', 'Lebanese', 'Hellinistic', 'Captian', 'Everything', 'Danelaw', 'Lower', 'Extra', 'Missionary', '800,000', 'Historians', 'Whats', 'Tragedy', 'Simon', 'Lack', 'atkins', 'Ashkenaz', 'Beyond', 'Bushy', 'Power', 'Oxygen', 'Athens', 'Elevator', 'Tres', 'Institutional', 'Domestic', 'Regardless', 'Miniature', 'Killer', 'Investment', 'Solar', 'Serving', 'And', 'Being', '4', 'Baptists', 'Freeman', 'Official', 'Four', 'Israeli', 'Children', 'after', 'Terrorists', 'Harff', 'Camillo', 'WHen', 'Sharks', 'Although', 'Karma', 'Short', 'St.', 'Domain', 'Damaged', 'Troiden', 'NICE', 'Ruins', 'Despite', 'Schools', 'Avoiding', 'North', 'Zn', 'Predator', 'Berlin', 'Prussia', 'Ethnohistory', 'Avicenna', 'Residence', 'Jesuits', 'Living', 'Genetic', 'Qing', 'rapid', 'Gray', 'Minority', 'Nature', 'Ashkenazi', 'Instrument', 'Etch-back', 'Cultural', 'SBC', 'lbert', 'Monks', 'Aonuma', 'Nanjing', 'Policymakers', 'HOw', 'Immunoassays', 'Poetry', 'Oak', 'Lester', 'Thermodynamics', 'Santa', 'Psychological', 'Man', 'Boyd', 'Masakado', 'Classic', 'Bain', 'Prontosil', 'Daylight', 'Alexander', 'Debuccalization', 'Westminster', 'Slovo', 'Gotamas', 'Melungeons', 'Cuarenta', 'Twelve', 'Julian', 'Secret', 'Nearly', 'Europe', 'Randomness', 'Individual', 'Viviparous', 'Tremont', 'Only', 'Narrow', 'Disparities', 'Lighting', 'dd', 'Kid', 'Padma', 'Greenpeace', 'Sometimes', 'Petrochemical', 'Food', 'Past', 'Fireworks', 'Unemployment', 'Chapultepec', 'Al-Andalus', 'Reduced', 'Pirie', 'Ancestors', 'HP-branded', 'Treating', 'Tamudic', 'Any', 'Hunting', 'Cardinal', '2', 'Bugs', 'Natural', 'Cape', 'Rawlinson', 'Finding', 'Adult', 'Beech', 'Op', 'Abilities', 'Bronze', 'Postmodern', 'Emotion', 'Plant', 'Recent', 'Feyman', 'Follicles', 'Reaction', 'Ancient', 'Vipassana', 'AME', 'Tomboy', 'hat', 'Starting', 'Wages', 'Whate', 'Basic', 'Quranic', 'Elam', 'enlish', 'Aviation', 'Euro1080', 'Asian', 'Channel', 'Pedro', 'Travel', 'Canadian', 'Bottom', 'Tengen', 'Relatives', 'Bagaru', 'UNFPA', 'Organisms', 'Golconda', 'Bohr', 'Targets', 'Bilaterians', 'Wifi', 'EIC', 'Submarines', 'Costs', 'Mutts', 'Why', 'Suffering', 'Roule', 'Masons', 'Steel', 'Projectiles', 'Analysis', 'Paraffin', 'Murders', 'Pectoralis', 'Commercial', 'Shmuel', 'Due', 'Media', 'Spectre', 'Cognitive', 'Universities', 'Roku', 'Shaping', 'Anti-infective', 'Stainless', 'Folk', 'Buddha', 'Navicella', 'Beyonce', 'R.', 'Increasing', 'Scandinavians', 'Poor', 'Morelos', 'Russia', 'Octaves', 'Vihuelas', 'Tendai', 'Calf', 'Discrepancy', 'Creatures', 'Tourism', 'Was', 'Non-standard', 'Taken', 'Nusach', 'Its', 'Anticircumvention', 'White', 'Dominican', 'Michigan', 'Barts', 'Whit', 'UK', 'Tufts-', 'Tarski', 'Pinning', 'Switzerland', 'Stringed', 'rather', 'Hg', 'Saqaliba', 'Namibia', 'Knesset', '181st', 'Sumerians', 'Readers', 'False', 'Reviewer', 'Lead', 'Non-standardized', 'Computer', 'Upright', 'Visual', 'Culturally', 'Self-psychology', 'Stimulants', 'Antiparticles', 'Barrage', 'Andreas', 'POJ', 'Alan', 'India', 'Nowadays', 'Preceding', 'Nations', 'Military', 'Earlier', 'High', 'Stain-resistant', 'Grande', 'IANA', 'Fake', 'Estonia', 'Renewables', 'Infinite', 'Atari', 'Persecution', 'Castro', 'why', 'Lay', 'Mantras', 'Colombier', 'ASTP', 'Sila', 'Anniversaries', 'Once', 'Around', 'Hydroelectricity', 'Hemimetabolous', 'Non-OTG', 'Iranians', 'Recylced', 'Consent', 'Smaller', 'Pomors', 'Buddism', 'Apple', 'Historically', 'Borough', 'Use', 'Churches', 'Leibnitz', 'Heat', 'Students', 'Decay', 'Castillan', 'Organophospates', 'Egypt', 'Anatomy', 'ICRISTAT', 'ZnO', 'Korean', 'Experts', 'Bessarabia', 'Species', 'Free-form', 'Conquests', 'Coffehouses', 'Sir', 'Ellis', 'Formation', 'Through', 'Local', 'House', 'Papyri', 'Houston', 'Farming', 'Winston', 'Mahiat', 'Trade', 'He', 'Mammals', 'Procopius', 'Outside', 'Peoples', 'Manolita', 'Whose', 'Burroughs-Wellcome', 'Air-dried', 'half', 'Discussions', 'Diderot', 'Indigenous', 'Diamond', 'Coins', 'Reticular', 'Gurian', 'Leon', 'Traits', 'Settlers', 'Baptist', 'Nagarjuna', 'Pasteurisation', 'Morphine', 'Friedrich', 'John', 'Lionel', 'Smyth', 'Horace', 'Commonwealth', 'Brandenburg', 'Convention', 'Control', 'Those', 'Captain', 'Confessional', 'Assyrian', 'Debutantes', 'Igbo', 'Repeatedly', 'Vinters', 'synthesizers', 'Whch', 'Currency', 'Assyria', 'Electronic', 'Rigas', 'Germany', 'ISA', 'Pre-war', 'Hundreds', 'Courts', 'Apollo', 'Elateridae', 'Bach', 'Milena', 'Goddard', 'Art', 'Galicia', 'ICBMs', 'Muffling', 'Slaveholders', 'Bermuda', 'Charles', 'Paideia', 'Sprint', 'Heian-kyo', 'La', 'Somali', 'Mutual', 'Whish', 'Migration', 'Sample', '114th', 'Investing', 'density', 'Smartzone', 'Olduwan', 'Such', 'Few', 'City', 'Traditional', 'Researchers', 'Serotonin', 'Global', 'Equestrianism', 'Eusocial', 'Macedon', 'Gajapati', 'Athenian', 'Tha', 'Francisco', 'IN', 'Carranza', 'Arsphenamine', 'Kant', 'Popper', 'Documents', 'Glial', 'Dromaeosaurids', 'Subjects', 'Pontus', 'According', 'Henry', 'Pollen', 'Something', 'Voice', 'About', 'Parque', 'Support', 'Reporting', 'WHn', 'Pain', 'Keith', 'Nirvana', 'Ismail', 'Raisina', 'Connaught', 'Decoders', 'Brass', 'Jan', 'Axons', 'UNtil', 'identify', 'Chemicals', 'Queens', 'Presbyterians', 'Sun', 'Near', '.What', 'MEG', 'Pure', 'Average', 'Admiration', 'Dinic', 'During', 'Geographically', 'Haredi', 'Rousseau', 'Afrotheria', 'Form', 'ABout', 'Additional', 'PC', 'Perfect', 'People', 'Holding', 'Montevideo', 'Antigen', 'PeeWee', 'Emissaries', 'May', 'Werner', 'Throughout', 'Passing', 'Roger', 'Bactria', 'West', 'Tom', 'Rayon', 'Money', 'Old', 'Relative', 'Performers', 'Diethylene', 'Encyclopedias', 'Shell', 'Basal', 'Particularly', 'Tin', 'DeCarlo', 'Liberating', 'Whhat', 'Paragraph', 'Contrary', 'Trying', 'Each', 'Carrefour', 'Attendance', 'Constantinople', 'Spinoza', 'Tajwid', 'Asymmetric', 'Hok-Lam', 'London-based', 'Pressure', 'Reactive', 'Zbigniew', 'Clusters', 'Immunology', 'Can', 'Avant-garde', 'Hyponatremia', 'Ibarra', 'Ireland', 'Hellenized', 'Unicode-based', 'Sri', 'Ethernet', 'Initially', 'Fictional', 'Trading', 'Babangida', 'Brown', 'Sending', 'Declaration', 'Aside', 'Ethiopia', 'Multiple', 'Hakka', 'Destiny', 'African', 'Government', 'Critics', 'Sameness', 'Pork', 'Precursors', 'Freemason', 'As', 'Laws', 'Hunter-gathering', 'Philip', 'Apprimately', 'Ṣalībī', 'Voiced', 'Reforms', 'Signing', 'Independent', 'Vibration-sensitive', 'Projects', 'Hysteria', 'Ivan', 'Willam', 'Nintendo', 'Park', 'UCL', 'Americo-Liberians', 'Crows', 'Variation', 'Impedance', 'Nationalist', 'Fat', 'Buddhism', 'England', 'Colloquial', 'Low-bandwidth', 'Modern-day', 'Beloved', '300', 'Fossils', 'Comics', 'Rounder', 'Neuroscience', 'More', 'GIving', 'Airburst', 'Origanum', 'Knibb', 'Drug', 'Hoklo', 'Athletes', 'above', 'Harmonics', 'Ban', 'Ney', 'marshall', 'Bactrian', 'Xinhua', 'Development', 'Whao', 'Some', 'Couts', 'Great', 'AT', 'Hipparchus', 'Developments', 'Serialization', 'Matter', 'Modern', 'Halvard', 'South', 'Locusts', 'Primordialist', 'Quantum', 'Diarrheal', 'Huge', \"'Melting\", 'Three', 'Durango', 'Given', 'Seditious', 'Oklahoma', 'Jacobson', 'Literary', 'Where', 'Omnipresent', 'Gallagher', 'Feminist', 'Tolimir', 'Animal', 'Ice', 'Companies', 'George', 'Mixing', 'Copper', 'Persians', 'Common', 'Paul', 'as', 'Uruguay', 'Placing', 'EPN', 'Counties', 'Alternative', 'Corporal', 'Tommy', 'Mephitis', 'Clarke', 'Continental', 'Pfizer', 'Bulgaria', 'Bushi', 'Protestants', 'Hans', 'Brincourt', 'Stress', 'Backwoods', 'LEDs', 'Hitting', 'Eudicots', 'Substandard', '1000', 'Atomically', 'Whittington', 'Neglect', 'Mrs.', 'Timur', 'Photo', 'Britain', 'Popcorn', '1080i', 'Eye', 'Commentated', 'Late', 'Diaz', 'Astrology', 'McWhorter', 'Karen', 'Russians', 'Nam', 'Appropriate', 'Vibrations', 'American', 'Corruption', 'Another', 'Byzantium', 'Human', 'Secretary', '16th', 'Internet', 'Anyone', 'Female', 'Mrigavyadha', 'Greater', 'Dalkom', 'Prajna', 'Bribes', 'PCR', 'With', 'PLO', 'August', 'Brain', 'Camoflauge', 'Coverage', 'Efforts', 'Avenue', 'Bob', 'Progressive', 'whats', 'Activity', 'Mechanically', 'Charlemagne', 'Lice', 'Bodhisattvas', 'Slavic', 'Scholars', 'how', 'Recognition', 'Hand-calculations', 'Travelling', 'YOutube', 'Somalia', 'Concepts', 'Patriarch', 'Slow', 'Wings', 'Rulers', 'Symbolically', 'ASCI', 'Sea', 'Soon', 'Lapis', 'Air', 'Australia', 'Pepsi', 'abbé', 'Ibrahim', 'Brazil', 'Requiring', 'Photodermatitis', 'Flooding', 'Rajasthan', 'Cappadocia', 'Aristotle', 'Mary', 'Biggeri', 'Of', 'Shramanas', 'Songhai', 'Post', 'Salla', 'In', 'Haydn', 'Caribean', 'Kiyomori', 'previous', 'Birth', 'SPSS', 'Mahbub', 'Elevation', 'Elena', 'Nietzsche', 'Paganism', 'Mennonites', 'Frequent', 'Zinc', 'Singapore', 'What', 'Union', 'Southampton', 'Matches', 'Whio', 'ho', 'Unicode', 'Immigrants', 'Radars', 'Queen', 'Netscape', 'President', 'Chopiniana', 'Octavian', 'Grand', 'Adherents', 'Kairomones', 'Brownstone', 'Neoptera', 'Hunted', 'Columbia', 'MP3', 'Totaling', 'Project', 'Jaipur', 'Powerful', 'DST', 'Features', 'Gothic', 'Home', 'Haplodiploidy', 'Describe', 'Little', 'Percy', 'Learning', 'Hugh', 'Ann', 'Dissociating', 'Kenneth', 'Along', 'Saint', 'Barracks', 'Whee', 'Science', 'Egyptianized', 'Sicily', 'Chitin', 'MDNA', 'Mindfulness', 'Several', 'between', 'Buses', 'Right', 'Historical', 'Antiochus', 'WBUR', 'Wadsworth', 'Diffusive', 'Composers', 'ASCII', 'Lawrence', 'Connecticut', 'Rumors', 'Thinkers', 'around', 'Bishop', 'Could', 'Homeostasis', 'Pita', 'Shuman', 'd', 'Marmontel', 'Firefox', 'Twice', 'Communists', 'Detroit', 'Ananda', 'Herbert', 'Piracy', 'Wy', 'Charities', 'Eyak', 'Nociceptors', 'AAI', 'Stallman', 'Water', 'Initiative', 'Sasha', 'Salam', 'Billions', 'Aove', 'We', 'Buckingham', 'Samsara', 'Plitical', 'Alive', 'Up', 'Boolean', 'Mycetophilldae', 'Maintaining', 'Delhi', 'Funding', 'Together', 'Wats', 'Carrier', 'Jean-Etienne', 'Cats', 'Spiderman', 'Mapping', 'Sky', '39.5', 'Jack', 'Videophone', 'Aluminium', 'Avani', 'Napoleon', 'Communications', 'Massive', 'Pop', 'Rational', 'Recipients', 'Someone', 'Security', 'Storyboards', '20th', 'Exercise-induced', 'Christoph', 'Holden', 'Einstein', 'French', 'Apis', 'Indoor', 'Tim', 'Captive', 'Richard', 'Lake', 'Western', 'Inhabitants', 'Chimamanda', 'Adrenalize', 'Tibetan', 'Representatives', 'Biological', '``', 'I-24', 'Females', 'Before', 'MediaOne', 'Cynanthus', 'Existing', 'Grow', 'Capello', 'DNA', 'renovations', 'Affiliates', 'Nara', 'Rock', 'Rita', 'Hidalgo', 'Discrimination', 'Protective', 'Prussian', 'Fellows', 'Regualtions', 'later', 'Fields', 'No', 'Asking', 'Niccolò', 'from', 'Pachelbel', 'Adolescent', 'Excessive', 'Multielectrode', 'Kosher', 'Intellectual', 'Offerings', 'Supporters', 'Duties', 'Facial', 'Teenagers', 'Rising', 'Aging', 'Second', 'Regions', 'Dry', 'Six', 'Isis', 'Uses', 'Higher', 'Friday', 'Whichgroup', 'Koch', 'Records', 'Materialism', 'Vessels', 'Electrodes', 'Low', 'Digital', 'King', 'Explorers', 'Permanent', 'Like', 'Cctavianus', 'Vaccinations', 'Scotty', 'Privilege', 'Political', 'Nicosulfuron', 'I', 'Britian', 'Reefs', 'Hellenistic', 'Corrupt', 'Cyberabad', 'Hawking', 'Michael', 'Flowers', 'Beside', 'Changing', 'Crazy', 'Incomplete', 'Parkscore', 'Does', 'Campidanese', 'When', 'Crimson', 'Urinating', 'Neo-classical', 'Should', 'Much', 'Stinger', 'Standard', 'Neil', 'Victor', 'Advertisers', 'Later', 'Miami', 'Followers', 'Edwin', 'Give', 'Debut', 'Neo-Georgain', 'Woyciechowski', '525', 'Tensions', 'Absolute', 'Nwankwo', 'IASP', '355', 'Peithon', 'Migratory', 'FireWire', 'Back-to-school', 'Material', 'Are', 'Roughly', 'Indian', 'Aquila', 'Technically', 'Fox', 'Hosts', 'States', 'Anti-aircraft', 'Gleick', 'dada', 'DeveloperWorks', 'Computing', 'Ascension', '1.1', 'early', 'Mystic', 'Analog', 'Maria', 'Heian', 'Paper', 'Known', 'Americans', 'Regarding', 'Emperors', 'Nobles', 'Predators', 'Including', 'All', 'Pins', 'Tylos', 'Dr.', 'about', 'Realms', 'Tribes', 'Contemporary', 'Saadia', 'Soft', 'Ghulam', 'Daylighting', 'Has', 'Synthpop', 'Valencia', 'Traffic', 'Environmental', 'Inspired', 'Sweating', 'Dialects', 'Mainstream', 'Articles', 'Erasmus', 'Alb', 'Originally', 'Absence', 'Playstation', 'Beetles', 'Processing', 'Tok', 'Disney', 'Height', 'Would', 'Mandarin', 'Micro-electro', 'Demotic', 'Language', 'Carthage', 'Technology', 'Al-Battani', 'Norepinephrine', 'Championships', 'Decisions', 'Lodges', 'Al-Biruni', 'Afrikaans', 'Reports', 'High-rise', 'Whic', 'Montesquieu', 'Maudlin', 'Hawaii', 'Setting', 'Dennis', 'Creole', 'Ogranisms', '2013', 'Production', 'This', 'Unittarians', 'Douay', 'Grave', 'Monopole', 'Teams', 'René', 'Marian', 'Printed', 'Games', 'Designers', 'Multi-color', 'Highest', 'Sauropsids', 'Louis', 'Multilateral', 'Lagrime', 'Pointers', 'It', 'Kerry', 'Noyce', 'Charged', 'Over', 'Zermelo', 'USB', 'W-VHS', 'Representations', 'Armenian', 'Parabolic', 'Automobile', 'Excitement', 'Kinship', 'Kakar', 'Rocks', 'Dionysus', 'Romanticism', '187th', 'Plymouth', 'Yaroslav', 'Kanmu', 'Barrel', 'Libelles', 'Research', 'Carl', 'Journals', 'Households', 'Trends', 'Two-fifths', 'Self-image', 'Dietary', 'Stewart', 'Carolina', 'Studying', 'Ossetic', 'Eavesdropping', 'Non-African', 'Origen', 'Thysanura', 'Resisting', 'Race', 'khorovïye', 'Whi', 'throughout', 'Most', 'Arabian', 'Traditionally', 'Victoria', 'Warner', 'Voters', 'Less', 'Oltenia', 'Passages', 'Wher', 'W', 'Reception', 'Since', 'Thalidomide', 'T3', 'Choules', 'Herero', '155th', 'Glaciars', 'Washington', 'Emperor', 'Albania', 'Employment', 'Greco-Macedonian', 'Proximity', 'Economic', 'Parts', 'Roman', 'Babson', 'Portugal', 'Church', 'Discussing', 'Chromosomes', 'Czech', 'Burma', 'Druze', 'Historic', 'Hong', 'Paleoptera', 'Socioeconomic', 'Define', 'On', 'Candidates', 'Buying', 'Theories', 'Springtails', 'FST', 'Supreme', 'Vis-à-vis', 'Island', 'Literature', 'Heroic', 'Voiceless', 'Cologne', 'Guests', 'Portions', 'Per', 'Allston', 'Hogarth', 'Heavier', 'Whar', 'Letter', 'Mach', 'Sanskrit', 'Palermo', 'Puritans', 'Daft', 'HTTP', 'Gunlaying', 'Encyclopedia', 'Vajrayana', 'Zener', '5000', 'ALl', 'Buddhists', 'Economist', 'Majority', 'Now', 'Legal', 'Dell', 'Monody', 'Abuse', 'Cluster', 'Grammar', 'Chinese', 'Adjusted', 'Canis', 'Countries', 'Stone', 'Emilie', 'Riverbank', 'Illegal', 'Tango', 'Craft', 'Suleiman', 'Rain', 'Tito', 'industrialization', 'Even', 'TV', 'Temperatures', 'Opposition', 'Users', 'Houuricanes', 'Silkworms', 'Whitehead', 'MP3Gain', 'Whithout', 'Gustavo', 'There', 'True', 'Slavic-speaking', 'Competition', 'Slaves', 'April', 'Having', 'Works', 'Kilims', 'Strict', 'Brains', 'Colonization', 'LED', 'Rhythm', 'Phonotactics', 'Mayday', 'Affluent', 'Assistance', 'Frank', 'Transistors', 'They', 'Precepts', 'Ducks', 'have', 'Mahayana', 'IS', 'Against', 'Niel', 'Hyderabad', 'Triploblastic', 'Myelin', 'Overtime', 'Geoffrey', 'Chorion', 'Unionists', 'Schumann', 'Worldwide', 'Iranian', 'Have', 'Risch', 'Edgar', 'Agencies', 'CBS', 'Tang', 'Article', 'Un', 'Ritchie', 'Blacks', 'Ghorala', 'Name', 'UCS', 'Characters', 'Juarez', 'Giovanni', 'Submarine', 'Hours', 'Neoplatonism', 'Tauton', 'Coercion', 'Latin', 'Urban', 'Layers', 'Whicy', 'USAF', 'TechRadar', 'Parliamentary', 'The', 'Ebreo', 'Maurice', 'ARPA', 'Governments', 'Artist', 'Large', 'BeDuhn', 'Author', 'Germans', 'Zyprexa', 'Statues', 'Garze', 'Islamic', 'NARA', 'Afonso', 'Glaciers', 'Devices', 'Construction', 'OTE', 'Motor', 'Credit', 'Ultimately', 'Character', 'Functioning', 'Def', 'Gumbe', 'Promulgation', 'Jonathon', 'Magna', 'Financial', 'Kings', 'Customarily', 'Odd-numbered', 'Islands', 'Jo', 'Above', 'Improvisation', 'Panter', 'Hellenism', 'Goldacre', 'Transition', 'Minimally', 'Tajikistan', 'Income', 'Identity', 'Distinction', 'OGIS', 'Sated', 'Eisenhower', 'Al-Farabi', 'Inserting', 'Asymmetry', 'Allowing', 'Pilote', 'Madero', 'Pesticides', 'Faith', 'Underground', 'Nehru', 'MBS', 'Photovoltaic', 'Ne', 'Cellular', 'Succession', 'Using', 'Inscriptions', 'Monastics', 'Spacial', 'aproximately', 'Whaen', 'Baroque', 'Agriculture', 'Terrestrial', 'Vibrato', 'Therapy', 'Leibniz', 'Lessons', 'Semiochemicals', 'Radar', 'Broad', 'Previous', 'Methodism', 'Plato', 'Lemming', 'Pitch', 'Movements', 'Because', 'Drivers', 'Excerpts', 'Certain', 'Methodists', 'Wooster', 'Desktop', 'Basso', 'Armies', 'a', 'Formerly', 'Arabia', 'Macau', 'Holland', 'Pullela', 'Illyrian', 'Every', 'Feynman', 'Places', 'Landings', 'Vegetation', 'AAC', 'Allotrophy', 'Green', 'Saltpetre', 'Wind', 'Quarks', 'Newborns', 'Fort', 'Democratic', 'Liverpool', 'Northern', 'Connecting', 'Handel', 'Loose', 'besides', 'Rosen', 'Filming', 'Unlike', 'Part', 'Politicians', 'Performance', 'Scotland', 'Crushing', 'Psychiatry', 'Downtown', 'Nicholas', 'Admiral', 'Pumped-storage', 'For', 'Primates', 'Mogadishu', 'Sinhalese', 'Pierre', 'Left', 'Mustaqbal', 'Massalia', 'Suddhodana', 'Half', 'Semitones', 'Regulations', 'Destabilization', 'Visitors', 'Presentism', 'Eastern', 'Boston', 'Ethnic', 'Life', 'Deshin', 'Krispy', 'SURE', 'Moderns', 'Shumer', 'Conservative', 'Periodic', 'Charleston', 'Minkowski', 'Polabian-Pomeranian', 'Grey', 'Anne', '54', 'Sounds', 'Justify', 'Wien', 'Albert', 'Elimination', 'Black', 'Advocates', 'Today', 'Witchety', 'Luminous', 'Surzhyk', 'Religious', 'Justifying', 'Cronyism', 'Individuals', 'Rationalism', 'Date', 'Missionaries', 'Polyphony', 'Based', 'ENIAC', 'Epping', 'Executive', 'Chlorothiazide', 'Kind', 'Others', 'Psycho-physical', 'British', 'Macedonia', 'Will', 'Cubists', 'Dogs', 'While', 'Congo', 'Neurogenesis', 'Dead', 'Reading', 'Buildings', 'First', 'Parties', 'Arts', 'Narrative', 'STC', 'Who', 'Bon', 'Atticus', 'Managers', 'Bees', 'Asthma', 'Capital', 'One', 'Bern', 'Attending', 'Unedited', 'Proto-Slavic', 'Hormuud', 'Pontian', 'Mount', 'Belize', 'Antarctica', 'Under', 'Wha', 'Endo', 'Geechee', 'one', 'Gherman', 'Wearing', 'Centrifugation', 'Newer', 'Moving', 'Adolescents', 'Hispanics', 'Normally', 'Corixids', 'Kalbeliya', 'Drosophila', 'Levels', 'SuperSpeed', 'Without', 'Rimer', 'by', 'Though', 'European', 'Burke', 'Airports', 'Jews', 'Governmental', 'Ronald', 'Apart', 'Groups', 'Tagalog', 'Germanic', 'Depending', 'Extremely', '*', 'Orozco', 'Computers', 'Vibrating', 'Métal', 'Firms', 'Elevations', 'Deflection', 'Farm', 'Understanding', 'New', 'Civilizations', 'Suga', 'VH1', 'Verse', 'UI', 'Webcomics', 'HD', 'Peru', 'Neal', 'Instead', 'Coinage', 'Boycotters', 'David', 'Electronically', 'Distinctive', 'Penumata', 'Phonology', 'Oral', 'At', 'Between', 'Dior', 'Classical', 'Iwo', 'Clothing', 'El', 'Usually', 'Johannes', 'Jewish', 'Perceiving', 'Jonathan', '2012', 'Hannibal', 'Jehovah', 'Billy', 'Street', 'Imports', 'Ibn', 'Yugoslavia', 'OCA', 'Belonging', 'Boundaries', 'Athanasius', 'Time', 'Dukkha', 'Passive', 'Community', 'Microsoft', 'Arab', 'PlayStation', 'Signals', 'Illegally', 'Good', 'Wood', 'Czechs', 'Joining', 'Separationists', 'Decision', 'Electromagnetic', 'Light-receptive', 'Liquid-fueled', 'Clouds', 'Israel', 'Overdependence', 'Yale', '808s', 'Magnifier', 'Engineering', 'Precipitation', 'Antennas', 'Nadiya', 'Stephen', 'Current', 'Spanish', '2000', 'Zari', 'Exposure', 'Livingston', 'Ursus', 'Pyometra', 'Integer', 'Cyprus', 'However', 'Mont', 'Fighting', 'Calculi', 'Obsidian', 'Theravada', 'Red', 'Led', 'Liberation', 'Hydrogen', 'Vocal', 'Easy', 'Consumer', 'War', 'Warriors', 'Ending', 'Malyarchuk', 'Artifacts', 'pʰ', 'Generally', 'Example', 'Howmany', 'Termites', 'Indentured', 'Petrarch', 'Y', 'Computational', 'Soul', 'Glam', 'Alloparenting', 'Deductive', 'Changes', 'Denis', 'Capitalization', 'Thomas', 'Translations', 'Descending', 'Testing', 'Listening', 'Armenia', 'News', 'SWAPO', 'J.S', 'Present', 'Whos', 'Akkadian', 'Die', 'Sergei', 'BDNF', 'Bronkhurst', 'Astronomy', 'Researcher', 'WHich', 'Rather', 'Fear', 'None', 'If', 'Lieberman', 'Visits', 'Halo', 'Duration', 'Men', 'Whoch', 'Schnitzel', 'Were', 'Madrasas', 'IBM', 'Gold', 'Full-bandwidth', 'Movement', 'General', 'Ganglia', 'Władysław', 'eSATA', 'Biblical', '1914', 'PepsiCo', 'Pentecostals', 'Laypersons', 'Petropavlovsk', 'Virgil', 'Giving', 'Trains', 'Obligations', 'Treaties', 'Immanuel', 'William', 'Bilingual', 'Waht', 'Sweden', 'Clark', 'Mamelon', 'Different', 'SD', 'Federal', 'Villa', 'Moltke', 'Literate', 'Christ', 'Perseus', 'Jazz', 'Approximentally', 'Programs', 'Dipoles', 'Interpretations', 'Diesel', 'Godel', 'MIT', 'Typhoons', 'ACLs', 'Arnold', 'Residents', 'Iran', 'Bostons', 'Immediately', 'Percentage', 'Diseases', 'is', 'Humoral', 'Kiss', 'Saxophone', 'Susa', 'MES-1', 'Soldiers', 'Invocation', 'Willett', 'Hemlock', 'Organochlorine', 'Into', 'Vegetarian', 'Serbs', 'WHat', 'Polyphonic', 'Their', 'Durkheim', 'Whatyear', 'Larry', 'Pan-Slavism', 'Royal', 'Peter', 'Orlam', 'wearing', 'Polish', 'Abraham', 'Markers', '`', 'Prepaid', 'Currently', 'Floyd', 'Hounds', 'Reader', 'Magnates', 'Fearing', 'Medieval', 'Below', 'Signatories', 'Motion', 'Attic', 'Downstream', 'Kukuzelis', 'Mizrahi', 'renaissance', 'Afraid', 'Pope', 'Till', 'Speakers', 'English', 'Fishing', 'Guinea-Bissau', 'Target', 'Hokkien', 'Continued', 'Blue', 'Violations', 'Whed', 'Assumptions', 'Georgian', 'Nutritional', 'Cossacks', 'Growing', 'the', 'Wolves', 'where', 'Assembly', 'Elements', 'Bootylicious', 'Providing', 'Reform', 'Fans', 'colloquial', 'Jay', 'WPXD', 'Dovrak', 'Cork', 'Early', 'Whan', 'Strasbourg', 'Bacterial', 'Greek', 'Pewter', 'Moths', 'Madonna', 'Go', 'Edward', 'Prudhoe', 'Guerra', 'Max', 'Ceremonies', 'Buyers', 'Going', 'Native', 'Heading', 'Input', 'Ingesting', 'Dish', 'Ho', 'Birds', 'Repetition', 'Sands', 'Poppers', 'Phengodidae', 'Doing', 'Maulana', 'Parental', 'Wang', 'Small', 'Softening', 'GMT', 'Rebetiko', 'Presbyterianism', 'Coupled', 'Next', 'WHy', 'Rainfall', 'Croatian', 'Organic', 'Poland', 'Complex', 'Supply', '50', 'Mohammad', 'Somerset', 'Browsers', 'Interplay', 'Portugese', 'Harper', 'Planck', 'Obesity', 'Portuguese', 'To', 'Smelter', 'Annealling', 'WHere', 'Simpson', 'Disk', 'Lie', 'Furniture', 'Discovery', 'Until', 'Pieces', 'Feral', 'Imamah', 'Estrella', 'Haplogroup', 'Serbo-Croatian', 'Balance', 'Steamships', 'Big', 'Coalition', 'Horn', 'Randomly', 'THE', 'Universal', 'Prices', 'Casual', 'Agassiz', 'Beyoncé', 'Voltaire', 'Kurt', 'Russian', 'Cladistics', 'Freemasonry', 'Rotation', 'Bilateral', 'Within', 'do', 'Is', 'Meninges', 'Conversations', 'Heavy', 'Wisdom', 'Psychiatrists', 'Avian', 'Cache', 'Emmett', 'Arthropods', 'Loops', 'CBC', 'Same-sex', 'UHI', 'Tahitian', 'Nigel', 'Sound', 'Constructing', 'Patients', 'Pancho', 'Aphis', 'Contestants', 'Deities', 'Justice', 'Many', 'Dean', 'Whixh', 'Guns', 'Cooler', 'Controversy', 'Benito', 'NYC', 'Asita', '/u/', 'Bi-racial', 'Almost', 'Studies', 'Paraneopter', 'Disease', 'Retailers', 'Prose', 'Marvel', 'Leaving', 'Opposing', 'Switching', 'GABA', 'Gregory', 'Images', 'Constitutional', 'Identify', 'Handshake', 'Squash', 'Carbon', 'Kanye', 'Respiration', 'Christians', 'Orontid', 'Broader', 'Greece', 'Lenin', 'Attalus', 'WWhat', 'Mason', '360', 'Central', 'Carnival', 'Did', 'Climate', 'SV40', 'Himachal', 'Broods', 'LiftEye', 'Increased', 'Book', 'Microchips', 'Afterwards', 'Dutch', 'Comic', 'Scientists', 'Hydrogens', 'avidya', 'Ephedrine', 'Goes', 'Congress', '8th', 'Maximizing', 'September', 'Pangaea', 'Fertilization', 'Drama', 'Safavid', 'Gran', 'Congressional', 'Hypersexual', 'Plaza', 'Close', 'ATC', 'Typically', 'Electricity', 'HTML', 'Hyperinsulinemia', 'Safety', 'Energy', 'Reflexes', 'Poorly', 'Software', 'Crassus', 'Recognizing', 'IP', 'Work', 'name', 'NYPD', 'Entrapment', 'Greeks', 'Closer', 'Pergamum', 'Retirees', 'Words', 'Matthew', 'Outer', 'TAFE', 'Whad', 'Two', 'Medical', 'Cabrini', 'Tasmania', 'Zeus', 'Translated', 'Sibley', 'Content', 'Writer', 'ACT', 'Prince', 'in', 'Irvin', 'Wa', 'Among', 'Lamps', 'Loud', 'Von', 'Upon', 'Jose', 'EMP', 'Comcast', 'on', 'Spain', 'Manuel'}\n",
      "2683\n"
     ]
    }
   ],
   "source": [
    "words_to_hide = 1\n",
    "\n",
    "hidden_merged = []\n",
    "\n",
    "for m in merged:\n",
    "    hidden = m.copy()\n",
    "    hidden[:words_to_hide] = [\"<qw>\"] * words_to_hide\n",
    "    hidden_merged.append(hidden)\n",
    "\n",
    "for i in range(5):\n",
    "    print()\n",
    "    print(\"Merged:\", merged[i])\n",
    "    print(\"Hidden:\", hidden_merged[i])\n",
    "\n",
    "interr_words = set()\n",
    "\n",
    "for questions in merged:\n",
    "    interr_words.update(questions[:words_to_hide])\n",
    "\n",
    "print()\n",
    "print(interr_words)\n",
    "print(len(interr_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_pred = [\n",
    "    ['What'],\n",
    "    ['When'],\n",
    "    ['Where'],\n",
    "    ['Who'],\n",
    "    ['Whom'],\n",
    "    ['In', 'what'],\n",
    "    ['In', 'which'],\n",
    "    ['Which'],\n",
    "    ['How', 'much'],\n",
    "    ['How', 'muany'],\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restricted_merged = []\n",
    "\n",
    "for questions in merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_words: ['1990s', 'late', 'the', 'in', '?', 'popular', 'becoming', 'start', 'Beyonce', 'did', 'When']\n",
      "Y_words: ['1990s', 'late', 'the', 'in', '?', 'popular', 'becoming', 'start', 'Beyonce', 'did', '<qw>']\n",
      "\n",
      "X_words: ['dancing', 'and', 'singing', '?', 'up', 'growing', 'was', 'she', 'when', 'in', 'compete', 'Beyonce', 'did', 'areas', 'What']\n",
      "Y_words: ['dancing', 'and', 'singing', '?', 'up', 'growing', 'was', 'she', 'when', 'in', 'compete', 'Beyonce', 'did', 'areas', '<qw>']\n",
      "\n",
      "X_words: ['2003', '?', 'singer', 'solo', 'a', 'become', 'and', 'Child', \"'s\", 'Destiny', 'leave', 'Beyonce', 'did', 'When']\n",
      "Y_words: ['2003', '?', 'singer', 'solo', 'a', 'become', 'and', 'Child', \"'s\", 'Destiny', 'leave', 'Beyonce', 'did', '<qw>']\n",
      "\n",
      "X_words: ['Texas', ',', 'Houston', '?', 'up', 'grow', 'Beyonce', 'did', 'state', 'and', 'city', 'what', 'In']\n",
      "Y_words: ['Texas', ',', 'Houston', '?', 'up', 'grow', 'Beyonce', 'did', 'state', 'and', 'city', 'what', '<qw>']\n",
      "\n",
      "X_words: ['1990s', 'late', '?', 'famous', 'become', 'Beyonce', 'did', 'decade', 'which', 'In']\n",
      "Y_words: ['1990s', 'late', '?', 'famous', 'become', 'Beyonce', 'did', 'decade', 'which', '<qw>']\n"
     ]
    }
   ],
   "source": [
    "x_words = [merged_question[::-1] for merged_question in merged]\n",
    "y_words = [hidden_answer[::-1] for hidden_answer in hidden_merged]\n",
    "\n",
    "for i in range(5):\n",
    "    print()\n",
    "    print(\"X_words:\", x_words[i])\n",
    "    print(\"Y_words:\", y_words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X: [21544, 48170, 49668, 31874, 31149, 3602, 14896, 1835, 21150, 12618, 65083]\n",
      "Y: [21544, 48170, 49668, 31874, 31149, 3602, 14896, 1835, 21150, 12618, 67298]\n",
      "\n",
      "X: [20898, 61937, 46659, 31149, 41438, 30710, 15960, 33499, 30236, 31874, 45462, 21150, 12618, 29146, 25588]\n",
      "Y: [20898, 61937, 46659, 31149, 41438, 30710, 15960, 33499, 30236, 31874, 45462, 21150, 12618, 29146, 67298]\n",
      "\n",
      "X: [23869, 31149, 41559, 66569, 59427, 4070, 61937, 16401, 45391, 41880, 61146, 21150, 12618, 65083]\n",
      "Y: [23869, 31149, 41559, 66569, 59427, 4070, 61937, 16401, 45391, 41880, 61146, 21150, 12618, 67298]\n",
      "\n",
      "X: [17821, 13572, 3213, 31149, 41438, 16864, 21150, 12618, 22201, 61937, 43286, 43050, 49514]\n",
      "Y: [17821, 13572, 3213, 31149, 41438, 16864, 21150, 12618, 22201, 61937, 43286, 43050, 67298]\n",
      "\n",
      "X: [21544, 48170, 31149, 31935, 4070, 21150, 12618, 1549, 66352, 49514]\n",
      "Y: [21544, 48170, 31149, 31935, 4070, 21150, 12618, 1549, 66352, 67298]\n"
     ]
    }
   ],
   "source": [
    "# Convert words to indices\n",
    "def words_to_indices(words):\n",
    "    return [word_to_idx[word] for word in words]\n",
    "\n",
    "\n",
    "x = [words_to_indices(words) for words in x_words]\n",
    "y = [words_to_indices(words) for words in y_words]\n",
    "\n",
    "for i in range(5):\n",
    "    print()\n",
    "    print(\"X:\", x[i])\n",
    "    print(\"Y:\", y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X: [21544, 48170, 49668, 31874, 31149, 3602, 14896, 1835, 21150, 12618, 65083, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299]\n",
      "Y: [21544, 48170, 49668, 31874, 31149, 3602, 14896, 1835, 21150, 12618, 67298, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299]\n",
      "\n",
      "X: [20898, 61937, 46659, 31149, 41438, 30710, 15960, 33499, 30236, 31874, 45462, 21150, 12618, 29146, 25588, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299]\n",
      "Y: [20898, 61937, 46659, 31149, 41438, 30710, 15960, 33499, 30236, 31874, 45462, 21150, 12618, 29146, 67298, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299]\n",
      "\n",
      "X: [23869, 31149, 41559, 66569, 59427, 4070, 61937, 16401, 45391, 41880, 61146, 21150, 12618, 65083, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299]\n",
      "Y: [23869, 31149, 41559, 66569, 59427, 4070, 61937, 16401, 45391, 41880, 61146, 21150, 12618, 67298, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299]\n",
      "\n",
      "X: [17821, 13572, 3213, 31149, 41438, 16864, 21150, 12618, 22201, 61937, 43286, 43050, 49514, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299]\n",
      "Y: [17821, 13572, 3213, 31149, 41438, 16864, 21150, 12618, 22201, 61937, 43286, 43050, 67298, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299]\n",
      "\n",
      "X: [21544, 48170, 31149, 31935, 4070, 21150, 12618, 1549, 66352, 49514, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299]\n",
      "Y: [21544, 48170, 31149, 31935, 4070, 21150, 12618, 1549, 66352, 67298, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299]\n"
     ]
    }
   ],
   "source": [
    "# Pad sequences\n",
    "max_len = max([len(words) for words in x])\n",
    "\n",
    "for i in range(len(x)):\n",
    "    x[i] += [word_to_idx[\"<pad>\"]] * (max_len - len(x[i]))\n",
    "    y[i] += [word_to_idx[\"<pad>\"]] * (max_len - len(y[i]))\n",
    "\n",
    "for i in range(5):\n",
    "    print()\n",
    "    print(\"X:\", x[i])\n",
    "    print(\"Y:\", y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensors\n",
    "x = torch.tensor(x).to(device)\n",
    "y = torch.tensor(y).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: ['1990s', 'late', 'the', 'in', '?', 'popular', 'becoming', 'start', 'Beyonce', 'did', 'When', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "Y: ['1990s', 'late', 'the', 'in', '?', 'popular', 'becoming', 'start', 'Beyonce', 'did', '<qw>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "# Example of a training pair (x, y) words\n",
    "i = 0\n",
    "print(\"X:\", [idx_to_word[idx.item()] for idx in x[i]])\n",
    "print(\"Y:\", [idx_to_word[idx.item()] for idx in y[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We will now our model to predict the first or the two first words of the question given the rest of the question and the answer. We will use a transformer model to do so.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 69456\n",
      "Val size: 8682\n",
      "Test size: 8683\n"
     ]
    }
   ],
   "source": [
    "# Train val test split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_val, y_val, test_size=0.5)\n",
    "\n",
    "print(\"Train size:\", len(x_train))\n",
    "print(\"Val size:\", len(x_val))\n",
    "print(\"Test size:\", len(x_test))\n",
    "\n",
    "# Create dataset\n",
    "\n",
    "\n",
    "class QuestionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "\n",
    "train_dataset = QuestionDataset(x_train, y_train)\n",
    "val_dataset = QuestionDataset(x_val, y_val)\n",
    "test_dataset = QuestionDataset(x_test, y_test)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM simple test model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94a0546009ec44fa8f9730434c9bf3f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca2f800445e94b6e82dcff07779ca63e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.9235, Val Loss: 0.3803\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "726fa57948174b38a2c1b399af26061c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Train Loss: 0.2385, Val Loss: 0.1969\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e4a2651054040f3919237e76a5c45c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Train Loss: 0.0996, Val Loss: 0.1361\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4a45f7d2efb4c30b30d39e8113e6cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Train Loss: 0.0379, Val Loss: 0.1178\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6631bc4d7fd748fd9327f7a303ba1d32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Train Loss: 0.0121, Val Loss: 0.1139\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "674bfe5a0b2848d4a1e53eb3e18d6dfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Train Loss: 0.0040, Val Loss: 0.1138\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c06b6cdcb6f4e378d96f08618370498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Train Loss: 0.0020, Val Loss: 0.1130\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccf405439eee4801b72c853822dabc94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Train Loss: 0.0014, Val Loss: 0.1114\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a972f27a34642259551dc0b0e729e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Train Loss: 0.0010, Val Loss: 0.1103\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e59ff91ab1144db8c6491d43e40eb1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Train Loss: 0.0007, Val Loss: 0.1099\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "class QuestionWordPredictor(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(QuestionWordPredictor, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 100\n",
    "\n",
    "model = QuestionWordPredictor(vocab_size, embedding_dim, hidden_dim).to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train model\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in trange(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for x_batch, y_batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x_batch)\n",
    "        loss = criterion(y_pred.view(-1, vocab_size), y_batch.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in val_loader:\n",
    "            y_pred = model(x_batch)\n",
    "            loss = criterion(y_pred.view(-1, vocab_size), y_batch.view(-1))\n",
    "            val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {\n",
    "          epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1114\n",
      "Train Accuracy: 0.9999\n",
      "Val Accuracy: 0.9925\n",
      "Test Accuracy: 0.9923\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        y_pred = model(x_batch)\n",
    "        loss = criterion(y_pred.view(-1, vocab_size), y_batch.view(-1))\n",
    "        test_loss += loss.item()\n",
    "    test_loss /= len(test_loader)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "def accuracy(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in loader:\n",
    "            y_pred = model(x_batch)\n",
    "            y_pred = y_pred.argmax(dim=-1)\n",
    "            correct += (y_pred == y_batch).sum().item()\n",
    "            total += y_batch.numel()\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "train_accuracy = accuracy(model, train_loader)\n",
    "val_accuracy = accuracy(model, val_loader)\n",
    "test_accuracy = accuracy(model, test_loader)\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Val Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.exists(\"models\"):\n",
    "    os.mkdir(\"models\")\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), \"models/question_word_predictor.pth\")\n",
    "\n",
    "# Load model\n",
    "model = QuestionWordPredictor(vocab_size, embedding_dim, hidden_dim).to(device)\n",
    "model.load_state_dict(torch.load(\"models/question_word_predictor.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: ['1963', '?', 'die', 'XXIII', 'John', 'Pope', 'did', 'year', 'what', 'In', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "Y: ['1963', '?', 'die', 'XXIII', 'John', 'Pope', 'did', 'year', 'what', '<qw>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "Y_pred: ['1963', '?', 'die', 'XXIII', 'John', 'Pope', 'did', 'year', 'what', '<qw>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "def predict(model, x):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(x)\n",
    "        y_pred = y_pred.argmax(dim=-1)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "# Example of a training pair (x, y) words\n",
    "i = 0\n",
    "x_i = x_train[i].unsqueeze(0)\n",
    "y_pred = predict(model, x_i)\n",
    "print(\"X:\", [idx_to_word[idx.item()] for idx in x_i[0]])\n",
    "print(\"Y:\", [idx_to_word[idx.item()] for idx in y_train[i]])\n",
    "print(\"Y_pred:\", [idx_to_word[idx.item()] for idx in y_pred[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
