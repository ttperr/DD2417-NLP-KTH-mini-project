{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question word prediction\n",
    "\n",
    "> Group 12: Tristan Perrot & Romain Darous\n",
    "\n",
    "Task is to train and evaluate a QWP model using any available QA-corpus, for instance, the [SQuAD corpus](https://rajpurkar.github.io/SQuAD-explorer/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA H100 80GB HBM3 MIG 1g.10gb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(torch.cuda.device_count()):\n",
    "    print(torch.cuda.get_device_properties(i).name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions: 86821\n",
      "\n",
      "Question: When did Beyonce start becoming popular?\n",
      "Answer: in the late 1990s\n",
      "\n",
      "Question: What areas did Beyonce compete in when she was growing up?\n",
      "Answer: singing and dancing\n",
      "\n",
      "Question: When did Beyonce leave Destiny's Child and become a solo singer?\n",
      "Answer: 2003\n",
      "\n",
      "Question: In what city and state did Beyonce  grow up? \n",
      "Answer: Houston, Texas\n",
      "\n",
      "Question: In which decade did Beyonce become famous?\n",
      "Answer: late 1990s\n"
     ]
    }
   ],
   "source": [
    "if data_dir not in os.listdir():\n",
    "    os.mkdir(data_dir)\n",
    "\n",
    "if \"squad_train.json\" not in os.listdir(data_dir):\n",
    "    # Download data at https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
    "    res = requests.get(\"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\")\n",
    "    data = json.loads(res.text)\n",
    "\n",
    "    # Save data to file\n",
    "    with open(data_dir + \"/squad_train.json\", \"w\") as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "with open(data_dir + \"/squad_train.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract answer text and question text\n",
    "answers = []\n",
    "questions = []\n",
    "for article in data[\"data\"]:\n",
    "    for paragraph in article[\"paragraphs\"]:\n",
    "        for qa in paragraph[\"qas\"]:\n",
    "            if qa[\"is_impossible\"]:\n",
    "                continue\n",
    "            answers.append(qa[\"answers\"][0][\"text\"])\n",
    "            questions.append(qa[\"question\"])\n",
    "\n",
    "print(\"Number of questions:\", len(questions))\n",
    "\n",
    "# Print some examples\n",
    "for i in range(5):\n",
    "    print()\n",
    "    print(\"Question:\", questions[i])\n",
    "    print(\"Answer:\", answers[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: ['When', 'did', 'Beyonce', 'start', 'becoming', 'popular', '?']\n",
      "Answer: ['in', 'the', 'late', '1990s']\n",
      "\n",
      "Question: ['What', 'areas', 'did', 'Beyonce', 'compete', 'in', 'when', 'she', 'was', 'growing', 'up', '?']\n",
      "Answer: ['singing', 'and', 'dancing']\n",
      "\n",
      "Question: ['When', 'did', 'Beyonce', 'leave', 'Destiny', \"'s\", 'Child', 'and', 'become', 'a', 'solo', 'singer', '?']\n",
      "Answer: ['2003']\n",
      "\n",
      "Question: ['In', 'what', 'city', 'and', 'state', 'did', 'Beyonce', 'grow', 'up', '?']\n",
      "Answer: ['Houston', ',', 'Texas']\n",
      "\n",
      "Question: ['In', 'which', 'decade', 'did', 'Beyonce', 'become', 'famous', '?']\n",
      "Answer: ['late', '1990s']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize questions\n",
    "tokenized_questions = [nltk.word_tokenize(q) for q in questions]\n",
    "\n",
    "# Tokenize answers\n",
    "tokenized_answers = [nltk.word_tokenize(a) for a in answers]\n",
    "\n",
    "for i in range(5):\n",
    "    print()\n",
    "    print(\"Question:\", tokenized_questions[i])\n",
    "    print(\"Answer:\", tokenized_answers[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merged: ['When', 'did', 'Beyonce', 'start', 'becoming', 'popular', '?', 'in', 'the', 'late', '1990s']\n",
      "\n",
      "Merged: ['What', 'areas', 'did', 'Beyonce', 'compete', 'in', 'when', 'she', 'was', 'growing', 'up', '?', 'singing', 'and', 'dancing']\n",
      "\n",
      "Merged: ['When', 'did', 'Beyonce', 'leave', 'Destiny', \"'s\", 'Child', 'and', 'become', 'a', 'solo', 'singer', '?', '2003']\n",
      "\n",
      "Merged: ['In', 'what', 'city', 'and', 'state', 'did', 'Beyonce', 'grow', 'up', '?', 'Houston', ',', 'Texas']\n",
      "\n",
      "Merged: ['In', 'which', 'decade', 'did', 'Beyonce', 'become', 'famous', '?', 'late', '1990s']\n"
     ]
    }
   ],
   "source": [
    "# Merge questions and answers\n",
    "merged = [q + a for q, a in zip(tokenized_questions, tokenized_answers)]\n",
    "\n",
    "for i in range(5):\n",
    "    print()\n",
    "    print(\"Merged:\", merged[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary\n",
    "vocab = set()\n",
    "for m in merged:\n",
    "    vocab.update(m)\n",
    "\n",
    "vocab = list(vocab)\n",
    "\n",
    "# Add \"<qw>\" to vocabulary\n",
    "vocab.append(\"<qw>\")\n",
    "vocab.append(\"<pad>\")\n",
    "\n",
    "# Create word to index and index to word mappings\n",
    "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "idx_to_word = {i: word for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merged: ['When', 'did', 'Beyonce', 'start', 'becoming', 'popular', '?', 'in', 'the', 'late', '1990s']\n",
      "Hidden: ['<qw>', 'did', 'Beyonce', 'start', 'becoming', 'popular', '?', 'in', 'the', 'late', '1990s']\n",
      "\n",
      "Merged: ['What', 'areas', 'did', 'Beyonce', 'compete', 'in', 'when', 'she', 'was', 'growing', 'up', '?', 'singing', 'and', 'dancing']\n",
      "Hidden: ['<qw>', 'areas', 'did', 'Beyonce', 'compete', 'in', 'when', 'she', 'was', 'growing', 'up', '?', 'singing', 'and', 'dancing']\n",
      "\n",
      "Merged: ['When', 'did', 'Beyonce', 'leave', 'Destiny', \"'s\", 'Child', 'and', 'become', 'a', 'solo', 'singer', '?', '2003']\n",
      "Hidden: ['<qw>', 'did', 'Beyonce', 'leave', 'Destiny', \"'s\", 'Child', 'and', 'become', 'a', 'solo', 'singer', '?', '2003']\n",
      "\n",
      "Merged: ['In', 'what', 'city', 'and', 'state', 'did', 'Beyonce', 'grow', 'up', '?', 'Houston', ',', 'Texas']\n",
      "Hidden: ['<qw>', 'what', 'city', 'and', 'state', 'did', 'Beyonce', 'grow', 'up', '?', 'Houston', ',', 'Texas']\n",
      "\n",
      "Merged: ['In', 'which', 'decade', 'did', 'Beyonce', 'become', 'famous', '?', 'late', '1990s']\n",
      "Hidden: ['<qw>', 'which', 'decade', 'did', 'Beyonce', 'become', 'famous', '?', 'late', '1990s']\n"
     ]
    }
   ],
   "source": [
    "words_to_hide = 1\n",
    "\n",
    "hidden_merged = []\n",
    "\n",
    "for m in merged:\n",
    "    hidden = m.copy()\n",
    "    hidden[:words_to_hide] = [\"<qw>\"] * words_to_hide\n",
    "    hidden_merged.append(hidden)\n",
    "\n",
    "for i in range(5):\n",
    "    print()\n",
    "    print(\"Merged:\", merged[i])\n",
    "    print(\"Hidden:\", hidden_merged[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_words: ['<qw>', 'did', 'Beyonce', 'start', 'becoming', 'popular', '?', 'in', 'the', 'late', '1990s']\n",
      "Y_words: ['When']\n",
      "\n",
      "X_words: ['<qw>', 'areas', 'did', 'Beyonce', 'compete', 'in', 'when', 'she', 'was', 'growing', 'up', '?', 'singing', 'and', 'dancing']\n",
      "Y_words: ['What']\n",
      "\n",
      "X_words: ['<qw>', 'did', 'Beyonce', 'leave', 'Destiny', \"'s\", 'Child', 'and', 'become', 'a', 'solo', 'singer', '?', '2003']\n",
      "Y_words: ['When']\n",
      "\n",
      "X_words: ['<qw>', 'what', 'city', 'and', 'state', 'did', 'Beyonce', 'grow', 'up', '?', 'Houston', ',', 'Texas']\n",
      "Y_words: ['In']\n",
      "\n",
      "X_words: ['<qw>', 'which', 'decade', 'did', 'Beyonce', 'become', 'famous', '?', 'late', '1990s']\n",
      "Y_words: ['In']\n"
     ]
    }
   ],
   "source": [
    "x_words = [[word for word in hidden] for hidden in hidden_merged]\n",
    "y_words = [word[:words_to_hide] for word in merged]\n",
    "\n",
    "for i in range(5):\n",
    "    print()\n",
    "    print(\"X_words:\", x_words[i])\n",
    "    print(\"Y_words:\", y_words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X: [67298, 11478, 15940, 10782, 14113, 19339, 34989, 4722, 57418, 39821, 6750]\n",
      "Y: [21451]\n",
      "\n",
      "X: [67298, 2207, 11478, 15940, 1336, 4722, 5463, 64400, 60670, 17175, 32925, 34989, 18917, 25901, 36169]\n",
      "Y: [649]\n",
      "\n",
      "X: [67298, 11478, 15940, 27858, 7760, 24722, 43865, 25901, 39001, 13807, 51515, 19829, 34989, 67002]\n",
      "Y: [21451]\n",
      "\n",
      "X: [67298, 48729, 51383, 25901, 1586, 11478, 15940, 48742, 32925, 34989, 33111, 56260, 61968]\n",
      "Y: [58777]\n",
      "\n",
      "X: [67298, 18872, 63046, 11478, 15940, 39001, 46589, 34989, 39821, 6750]\n",
      "Y: [58777]\n"
     ]
    }
   ],
   "source": [
    "# Convert words to indices\n",
    "def words_to_indices(words):\n",
    "    return [word_to_idx[word] for word in words]\n",
    "\n",
    "x = [words_to_indices(words) for words in x_words]\n",
    "y = [words_to_indices(words) for words in y_words]\n",
    "\n",
    "for i in range(5):\n",
    "    print()\n",
    "    print(\"X:\", x[i])\n",
    "    print(\"Y:\", y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X: [67298, 11478, 15940, 10782, 14113, 19339, 34989, 4722, 57418, 39821, 6750, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299]\n",
      "Y: [21451]\n",
      "\n",
      "X: [67298, 2207, 11478, 15940, 1336, 4722, 5463, 64400, 60670, 17175, 32925, 34989, 18917, 25901, 36169, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299]\n",
      "Y: [649]\n",
      "\n",
      "X: [67298, 11478, 15940, 27858, 7760, 24722, 43865, 25901, 39001, 13807, 51515, 19829, 34989, 67002, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299]\n",
      "Y: [21451]\n",
      "\n",
      "X: [67298, 48729, 51383, 25901, 1586, 11478, 15940, 48742, 32925, 34989, 33111, 56260, 61968, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299]\n",
      "Y: [58777]\n",
      "\n",
      "X: [67298, 18872, 63046, 11478, 15940, 39001, 46589, 34989, 39821, 6750, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299]\n",
      "Y: [58777]\n"
     ]
    }
   ],
   "source": [
    "# Pad sequences\n",
    "max_len = max([len(words) for words in x])\n",
    "\n",
    "for i in range(len(x)):\n",
    "    x[i] += [word_to_idx[\"<pad>\"]] * (max_len - len(x[i]))\n",
    "\n",
    "for i in range(5):\n",
    "    print()\n",
    "    print(\"X:\", x[i])\n",
    "    print(\"Y:\", y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X: [67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 6750, 39821, 57418, 4722, 34989, 19339, 14113, 10782, 15940, 11478, 67298]\n",
      "Y: [21451]\n",
      "\n",
      "X: [67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 36169, 25901, 18917, 34989, 32925, 17175, 60670, 64400, 5463, 4722, 1336, 15940, 11478, 2207, 67298]\n",
      "Y: [649]\n",
      "\n",
      "X: [67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67002, 34989, 19829, 51515, 13807, 39001, 25901, 43865, 24722, 7760, 27858, 15940, 11478, 67298]\n",
      "Y: [21451]\n",
      "\n",
      "X: [67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 61968, 56260, 33111, 34989, 32925, 48742, 15940, 11478, 1586, 25901, 51383, 48729, 67298]\n",
      "Y: [58777]\n",
      "\n",
      "X: [67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 6750, 39821, 34989, 46589, 39001, 15940, 11478, 63046, 18872, 67298]\n",
      "Y: [58777]\n"
     ]
    }
   ],
   "source": [
    "# Reverse the order of the input sequence\n",
    "x = [x_i[::-1] for x_i in x]\n",
    "y = [y_i[::-1] for y_i in y]\n",
    "\n",
    "for i in range(5):\n",
    "    print()\n",
    "    print(\"X:\", x[i])\n",
    "    print(\"Y:\", y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensors\n",
    "x = torch.tensor(x).to(device)\n",
    "y = torch.tensor(y).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We will now our model to predict the first or the two first words of the question given the rest of the question and the answer. We will use a transformer model to do so.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM simple test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 69456\n",
      "Val size: 8682\n",
      "Test size: 8683\n"
     ]
    }
   ],
   "source": [
    "# Train val test split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_val, y_val, test_size=0.5)\n",
    "\n",
    "print(\"Train size:\", len(x_train))\n",
    "print(\"Val size:\", len(x_val))\n",
    "print(\"Test size:\", len(x_test))\n",
    "\n",
    "# Create dataset\n",
    "class WordDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "\n",
    "train_dataset = WordDataset(x_train, y_train)\n",
    "val_dataset = WordDataset(x_val, y_val)\n",
    "test_dataset = WordDataset(x_test, y_test)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76856f76142c46db817609855c95d6a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37765c75cc6d4915982cf9d2caca5ac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "NVML_SUCCESS == r INTERNAL ASSERT FAILED at \"../c10/cuda/CUDACachingAllocator.cpp\":1154, please report a bug to PyTorch. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader):\n\u001b[1;32m     37\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 38\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m y_pred\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab_size)\n\u001b[1;32m     40\u001b[0m     y_batch \u001b[38;5;241m=\u001b[39m y_batch\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 12\u001b[0m, in \u001b[0;36mNextWordModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)\n\u001b[1;32m     11\u001b[0m x, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(x)\n\u001b[0;32m---> 12\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: NVML_SUCCESS == r INTERNAL ASSERT FAILED at \"../c10/cuda/CUDACachingAllocator.cpp\":1154, please report a bug to PyTorch. "
     ]
    }
   ],
   "source": [
    "# Define model of predicting next word\n",
    "class NextWordModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(NextWordModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "# Define model\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 128\n",
    "\n",
    "model = NextWordModel(vocab_size, embedding_dim, hidden_dim).to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train model\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in trange(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    for x_batch, y_batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x_batch)\n",
    "        y_pred = y_pred.view(-1, vocab_size)\n",
    "        y_batch = y_batch.view(-1)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        train_total += y_batch.size(0)\n",
    "        train_correct += (y_pred.argmax(dim=1) == y_batch).sum().item()\n",
    "    train_accuracy = train_correct / train_total\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in tqdm(val_loader):\n",
    "            y_pred = model(x_batch)\n",
    "            y_pred = y_pred.view(-1, vocab_size)\n",
    "            y_batch = y_batch.view(-1)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            val_loss += loss.item()\n",
    "            val_total += y_batch.size(0)\n",
    "            val_correct += (y_pred.argmax(dim=1) == y_batch).sum().item()\n",
    "    val_accuracy = val_correct / val_total\n",
    "\n",
    "    print(\"Epoch:\", epoch + 1)\n",
    "    print(\"Train loss:\", train_loss / len(train_loader))\n",
    "    print(\"Train accuracy:\", train_accuracy)\n",
    "    print(\"Val loss:\", val_loss / len(val_loader))\n",
    "    print(\"Val accuracy:\", val_accuracy)\n",
    "\n",
    "# Test model\n",
    "model.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in tqdm(test_loader):\n",
    "        y_pred = model(x_batch)\n",
    "        y_pred = y_pred.view(-1, vocab_size)\n",
    "        y_batch = y_batch.view(-1)\n",
    "        test_correct += (y_pred.argmax(dim=1) == y_batch).sum().item()\n",
    "        test_total += y_batch.size(0)\n",
    "test_accuracy = test_correct / test_total\n",
    "\n",
    "print(\"Test accuracy:\", test_accuracy)\n",
    "\n",
    "# Predict next word\n",
    "def predict_question_word(question):\n",
    "    question = nltk.word_tokenize(question)\n",
    "    question = [\"<qw>\"] + question\n",
    "    question = [word_to_idx[word] for word in question]\n",
    "    # Reverse the order of the input sequence\n",
    "    question = question[::-1]\n",
    "    question = torch.tensor(question).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(question)\n",
    "    y_pred = y_pred.squeeze(0)\n",
    "    y_pred = y_pred[-1]\n",
    "    y_pred = F.softmax(y_pred, dim=0)\n",
    "    y_pred = y_pred.cpu().numpy()\n",
    "    y_pred = y_pred.argsort()[::-1]\n",
    "    y_pred = [idx_to_word[idx] for idx in y_pred]\n",
    "    return y_pred\n",
    "\n",
    "sentence = \"<qw> is the capital of France? Paris\"\n",
    "predict_question_word(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63ae4794624a435b84b69cf655181378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/272 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9947\n",
      "\n",
      "True: <pad>\n",
      "Pred: <pad>\n",
      "\n",
      "True: <pad>\n",
      "Pred: <pad>\n",
      "\n",
      "True: <pad>\n",
      "Pred: <pad>\n",
      "\n",
      "True: <pad>\n",
      "Pred: <pad>\n",
      "\n",
      "True: <pad>\n",
      "Pred: <pad>\n",
      "\n",
      "True: <pad>\n",
      "Pred: <pad>\n",
      "\n",
      "True: <pad>\n",
      "Pred: <pad>\n",
      "\n",
      "True: <pad>\n",
      "Pred: <pad>\n",
      "\n",
      "True: <pad>\n",
      "Pred: <pad>\n",
      "\n",
      "True: <pad>\n",
      "Pred: <pad>\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation on test set\n",
    "\n",
    "# True and predicted words\n",
    "true_words = []\n",
    "pred_words = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in tqdm(test_loader):\n",
    "        y_pred = model(x_batch)\n",
    "        y_pred = torch.argmax(y_pred, dim=2).cpu().numpy()\n",
    "        for y_true, y_p in zip(y_batch.cpu().numpy(), y_pred):\n",
    "            true_words.append([idx_to_word[idx] for idx in y_true])\n",
    "            pred_words.append([idx_to_word[idx] for idx in y_p])\n",
    "\n",
    "# Flatten lists\n",
    "true_words = [word for words in true_words for word in words]\n",
    "pred_words = [word for words in pred_words for word in words]\n",
    "\n",
    "# Calculate accuracy\n",
    "correct = sum([1 for t, p in zip(true_words, pred_words) if t == p])\n",
    "accuracy = correct / len(true_words)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Randomly sample 10 examples\n",
    "for i in range(10):\n",
    "    idx = torch.randint(len(true_words), (1,)).item()\n",
    "    print()\n",
    "    print(\"True:\", true_words[idx])\n",
    "    print(\"Pred:\", pred_words[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory models does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Save model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/word_predictor_lstm_1st.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m WordPredictor(vocab_size, embedding_dim, hidden_dim)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:618\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    615\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 618\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    619\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[1;32m    620\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:492\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    491\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:463\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream))\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Parent directory models does not exist."
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"models\"):\n",
    "    os.mkdir(\"models\")\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), \"models/word_predictor_lstm_1st.pth\")\n",
    "\n",
    "# Load model\n",
    "model = WordPredictor(vocab_size, embedding_dim, hidden_dim).to(device)\n",
    "model.load_state_dict(torch.load(\"models/word_predictor_lstm_1st.pth\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
