{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question word prediction\n",
    "\n",
    "> Group 12: Tristan Perrot & Romain Darous\n",
    "\n",
    "Task is to train and evaluate a QWP model using any available QA-corpus, for instance, the [SQuAD corpus](https://rajpurkar.github.io/SQuAD-explorer/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(torch.cuda.device_count()):\n",
    "    print(torch.cuda.get_device_properties(i).name)\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions: 86821\n",
      "\n",
      "Question: When did Beyonce start becoming popular?\n",
      "Answer: in the late 1990s\n",
      "\n",
      "Question: What areas did Beyonce compete in when she was growing up?\n",
      "Answer: singing and dancing\n",
      "\n",
      "Question: When did Beyonce leave Destiny's Child and become a solo singer?\n",
      "Answer: 2003\n",
      "\n",
      "Question: In what city and state did Beyonce  grow up? \n",
      "Answer: Houston, Texas\n",
      "\n",
      "Question: In which decade did Beyonce become famous?\n",
      "Answer: late 1990s\n"
     ]
    }
   ],
   "source": [
    "if data_dir not in os.listdir():\n",
    "    os.mkdir(data_dir)\n",
    "\n",
    "if \"squad_train.json\" not in os.listdir(data_dir):\n",
    "    # Download data at https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
    "    res = requests.get(\"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\")\n",
    "    data = json.loads(res.text)\n",
    "\n",
    "    # Save data to file\n",
    "    with open(data_dir + \"/squad_train.json\", \"w\") as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "with open(data_dir + \"/squad_train.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract answer text and question text\n",
    "answers = []\n",
    "questions = []\n",
    "for article in data[\"data\"]:\n",
    "    for paragraph in article[\"paragraphs\"]:\n",
    "        for qa in paragraph[\"qas\"]:\n",
    "            if qa[\"is_impossible\"]:\n",
    "                continue\n",
    "            answers.append(qa[\"answers\"][0][\"text\"])\n",
    "            questions.append(qa[\"question\"])\n",
    "\n",
    "print(\"Number of questions:\", len(questions))\n",
    "\n",
    "# Print some examples\n",
    "for i in range(5):\n",
    "    print()\n",
    "    print(\"Question:\", questions[i])\n",
    "    print(\"Answer:\", answers[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: ['When', 'did', 'Beyonce', 'start', 'becoming', 'popular', '?']\n",
      "Answer: ['in', 'the', 'late', '1990s']\n",
      "\n",
      "Question: ['What', 'areas', 'did', 'Beyonce', 'compete', 'in', 'when', 'she', 'was', 'growing', 'up', '?']\n",
      "Answer: ['singing', 'and', 'dancing']\n",
      "\n",
      "Question: ['When', 'did', 'Beyonce', 'leave', 'Destiny', \"'s\", 'Child', 'and', 'become', 'a', 'solo', 'singer', '?']\n",
      "Answer: ['2003']\n",
      "\n",
      "Question: ['In', 'what', 'city', 'and', 'state', 'did', 'Beyonce', 'grow', 'up', '?']\n",
      "Answer: ['Houston', ',', 'Texas']\n",
      "\n",
      "Question: ['In', 'which', 'decade', 'did', 'Beyonce', 'become', 'famous', '?']\n",
      "Answer: ['late', '1990s']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize questions\n",
    "tokenized_questions = [nltk.word_tokenize(q) for q in questions]\n",
    "\n",
    "# Tokenize answers\n",
    "tokenized_answers = [nltk.word_tokenize(a) for a in answers]\n",
    "\n",
    "for i in range(5):\n",
    "    print()\n",
    "    print(\"Question:\", tokenized_questions[i])\n",
    "    print(\"Answer:\", tokenized_answers[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merged: ['When', 'did', 'Beyonce', 'start', 'becoming', 'popular', '?', 'in', 'the', 'late', '1990s']\n",
      "\n",
      "Merged: ['What', 'areas', 'did', 'Beyonce', 'compete', 'in', 'when', 'she', 'was', 'growing', 'up', '?', 'singing', 'and', 'dancing']\n",
      "\n",
      "Merged: ['When', 'did', 'Beyonce', 'leave', 'Destiny', \"'s\", 'Child', 'and', 'become', 'a', 'solo', 'singer', '?', '2003']\n",
      "\n",
      "Merged: ['In', 'what', 'city', 'and', 'state', 'did', 'Beyonce', 'grow', 'up', '?', 'Houston', ',', 'Texas']\n",
      "\n",
      "Merged: ['In', 'which', 'decade', 'did', 'Beyonce', 'become', 'famous', '?', 'late', '1990s']\n"
     ]
    }
   ],
   "source": [
    "# Merge questions and answers\n",
    "merged = [q + a for q, a in zip(tokenized_questions, tokenized_answers)]\n",
    "\n",
    "for i in range(5):\n",
    "    print()\n",
    "    print(\"Merged:\", merged[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary\n",
    "vocab = set()\n",
    "for m in merged:\n",
    "    vocab.update(m)\n",
    "\n",
    "vocab = list(vocab)\n",
    "\n",
    "# Add \"<qw>\" to vocabulary\n",
    "vocab.append(\"<qw>\")\n",
    "\n",
    "# Create word to index and index to word mappings\n",
    "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "idx_to_word = {i: word for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merged: ['When', 'did', 'Beyonce', 'start', 'becoming', 'popular', '?', 'in', 'the', 'late', '1990s']\n",
      "Hidden: ['<qw>', '<qw>', 'Beyonce', 'start', 'becoming', 'popular', '?', 'in', 'the', 'late', '1990s']\n",
      "\n",
      "Merged: ['What', 'areas', 'did', 'Beyonce', 'compete', 'in', 'when', 'she', 'was', 'growing', 'up', '?', 'singing', 'and', 'dancing']\n",
      "Hidden: ['<qw>', '<qw>', 'did', 'Beyonce', 'compete', 'in', 'when', 'she', 'was', 'growing', 'up', '?', 'singing', 'and', 'dancing']\n",
      "\n",
      "Merged: ['When', 'did', 'Beyonce', 'leave', 'Destiny', \"'s\", 'Child', 'and', 'become', 'a', 'solo', 'singer', '?', '2003']\n",
      "Hidden: ['<qw>', '<qw>', 'Beyonce', 'leave', 'Destiny', \"'s\", 'Child', 'and', 'become', 'a', 'solo', 'singer', '?', '2003']\n",
      "\n",
      "Merged: ['In', 'what', 'city', 'and', 'state', 'did', 'Beyonce', 'grow', 'up', '?', 'Houston', ',', 'Texas']\n",
      "Hidden: ['<qw>', '<qw>', 'city', 'and', 'state', 'did', 'Beyonce', 'grow', 'up', '?', 'Houston', ',', 'Texas']\n",
      "\n",
      "Merged: ['In', 'which', 'decade', 'did', 'Beyonce', 'become', 'famous', '?', 'late', '1990s']\n",
      "Hidden: ['<qw>', '<qw>', 'decade', 'did', 'Beyonce', 'become', 'famous', '?', 'late', '1990s']\n"
     ]
    }
   ],
   "source": [
    "words_to_hide = 2\n",
    "\n",
    "hidden_merged = []\n",
    "\n",
    "for m in merged:\n",
    "    hidden = m.copy()\n",
    "    hidden[:words_to_hide] = [\"<qw>\"] * words_to_hide\n",
    "    hidden_merged.append(hidden)\n",
    "\n",
    "for i in range(5):\n",
    "    print()\n",
    "    print(\"Merged:\", merged[i])\n",
    "    print(\"Hidden:\", hidden_merged[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We will now our model to predict the first or the two first words of the question given the rest of the question and the answer. We will use a transformer model to do so.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
