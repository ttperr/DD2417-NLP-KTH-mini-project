{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question word prediction\n",
    "\n",
    "> Group 12: Tristan Perrot & Romain Darous\n",
    "\n",
    "Task is to train and evaluate a QWP model using any available QA-corpus, for instance, the [SQuAD corpus](https://rajpurkar.github.io/SQuAD-explorer/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA H100 80GB HBM3 MIG 1g.10gb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(torch.cuda.device_count()):\n",
    "    print(torch.cuda.get_device_properties(i).name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available(\n",
    ") else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions: 86821\n",
      "\n",
      "Question: When did Beyonce start becoming popular?\n",
      "Answer: in the late 1990s\n",
      "\n",
      "Question: What areas did Beyonce compete in when she was growing up?\n",
      "Answer: singing and dancing\n",
      "\n",
      "Question: When did Beyonce leave Destiny's Child and become a solo singer?\n",
      "Answer: 2003\n",
      "\n",
      "Question: In what city and state did Beyonce  grow up? \n",
      "Answer: Houston, Texas\n",
      "\n",
      "Question: In which decade did Beyonce become famous?\n",
      "Answer: late 1990s\n"
     ]
    }
   ],
   "source": [
    "if data_dir not in os.listdir():\n",
    "    os.mkdir(data_dir)\n",
    "\n",
    "if \"squad_train.json\" not in os.listdir(data_dir):\n",
    "    # Download data at https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
    "    res = requests.get(\n",
    "        \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\")\n",
    "    data = json.loads(res.text)\n",
    "\n",
    "    # Save data to file\n",
    "    with open(data_dir + \"/squad_train.json\", \"w\") as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "with open(data_dir + \"/squad_train.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract answer text and question text\n",
    "answers = []\n",
    "questions = []\n",
    "for article in data[\"data\"]:\n",
    "    for paragraph in article[\"paragraphs\"]:\n",
    "        for qa in paragraph[\"qas\"]:\n",
    "            if qa[\"is_impossible\"]:\n",
    "                continue\n",
    "            answers.append(qa[\"answers\"][0][\"text\"])\n",
    "            questions.append(qa[\"question\"])\n",
    "\n",
    "print(\"Number of questions:\", len(questions))\n",
    "\n",
    "# Print some examples\n",
    "for i in range(5):\n",
    "    print()\n",
    "    print(\"Question:\", questions[i])\n",
    "    print(\"Answer:\", answers[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: ['When', 'did', 'Beyonce', 'start', 'becoming', 'popular', '?']\n",
      "Answer: ['in', 'the', 'late', '1990s']\n",
      "\n",
      "Question: ['What', 'areas', 'did', 'Beyonce', 'compete', 'in', 'when', 'she', 'was', 'growing', 'up', '?']\n",
      "Answer: ['singing', 'and', 'dancing']\n",
      "\n",
      "Question: ['When', 'did', 'Beyonce', 'leave', 'Destiny', \"'s\", 'Child', 'and', 'become', 'a', 'solo', 'singer', '?']\n",
      "Answer: ['2003']\n",
      "\n",
      "Question: ['In', 'what', 'city', 'and', 'state', 'did', 'Beyonce', 'grow', 'up', '?']\n",
      "Answer: ['Houston', ',', 'Texas']\n",
      "\n",
      "Question: ['In', 'which', 'decade', 'did', 'Beyonce', 'become', 'famous', '?']\n",
      "Answer: ['late', '1990s']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize questions\n",
    "tokenized_questions = [nltk.word_tokenize(q) for q in questions]\n",
    "\n",
    "# Tokenize answers\n",
    "tokenized_answers = [nltk.word_tokenize(a) for a in answers]\n",
    "\n",
    "for i in range(5):\n",
    "    print()\n",
    "    print(\"Question:\", tokenized_questions[i])\n",
    "    print(\"Answer:\", tokenized_answers[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merged: ['When', 'did', 'Beyonce', 'start', 'becoming', 'popular', '?', 'in', 'the', 'late', '1990s']\n",
      "\n",
      "Merged: ['What', 'areas', 'did', 'Beyonce', 'compete', 'in', 'when', 'she', 'was', 'growing', 'up', '?', 'singing', 'and', 'dancing']\n",
      "\n",
      "Merged: ['When', 'did', 'Beyonce', 'leave', 'Destiny', \"'s\", 'Child', 'and', 'become', 'a', 'solo', 'singer', '?', '2003']\n",
      "\n",
      "Merged: ['In', 'what', 'city', 'and', 'state', 'did', 'Beyonce', 'grow', 'up', '?', 'Houston', ',', 'Texas']\n",
      "\n",
      "Merged: ['In', 'which', 'decade', 'did', 'Beyonce', 'become', 'famous', '?', 'late', '1990s']\n"
     ]
    }
   ],
   "source": [
    "# Merge questions and answers\n",
    "merged = [q + a for q, a in zip(tokenized_questions, tokenized_answers)]\n",
    "\n",
    "for i in range(5):\n",
    "    print()\n",
    "    print(\"Merged:\", merged[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary\n",
    "vocab = set()\n",
    "for m in merged:\n",
    "    vocab.update(m)\n",
    "\n",
    "vocab = list(vocab)\n",
    "\n",
    "# Add \"<qw>\" to vocabulary\n",
    "vocab.append(\"<qw>\")\n",
    "vocab.append(\"<pad>\")\n",
    "\n",
    "# Create word to index and index to word mappings\n",
    "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "idx_to_word = {i: word for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merged: ['When', 'did', 'Beyonce', 'start', 'becoming', 'popular', '?', 'in', 'the', 'late', '1990s']\n",
      "Hidden: ['<qw>', 'did', 'Beyonce', 'start', 'becoming', 'popular', '?', 'in', 'the', 'late', '1990s']\n",
      "\n",
      "Merged: ['What', 'areas', 'did', 'Beyonce', 'compete', 'in', 'when', 'she', 'was', 'growing', 'up', '?', 'singing', 'and', 'dancing']\n",
      "Hidden: ['<qw>', 'areas', 'did', 'Beyonce', 'compete', 'in', 'when', 'she', 'was', 'growing', 'up', '?', 'singing', 'and', 'dancing']\n",
      "\n",
      "Merged: ['When', 'did', 'Beyonce', 'leave', 'Destiny', \"'s\", 'Child', 'and', 'become', 'a', 'solo', 'singer', '?', '2003']\n",
      "Hidden: ['<qw>', 'did', 'Beyonce', 'leave', 'Destiny', \"'s\", 'Child', 'and', 'become', 'a', 'solo', 'singer', '?', '2003']\n",
      "\n",
      "Merged: ['In', 'what', 'city', 'and', 'state', 'did', 'Beyonce', 'grow', 'up', '?', 'Houston', ',', 'Texas']\n",
      "Hidden: ['<qw>', 'what', 'city', 'and', 'state', 'did', 'Beyonce', 'grow', 'up', '?', 'Houston', ',', 'Texas']\n",
      "\n",
      "Merged: ['In', 'which', 'decade', 'did', 'Beyonce', 'become', 'famous', '?', 'late', '1990s']\n",
      "Hidden: ['<qw>', 'which', 'decade', 'did', 'Beyonce', 'become', 'famous', '?', 'late', '1990s']\n"
     ]
    }
   ],
   "source": [
    "words_to_hide = 1\n",
    "\n",
    "hidden_merged = []\n",
    "\n",
    "for m in merged:\n",
    "    hidden = m.copy()\n",
    "    hidden[:words_to_hide] = [\"<qw>\"] * words_to_hide\n",
    "    hidden_merged.append(hidden)\n",
    "\n",
    "for i in range(5):\n",
    "    print()\n",
    "    print(\"Merged:\", merged[i])\n",
    "    print(\"Hidden:\", hidden_merged[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_words: ['1990s', 'late', 'the', 'in', '?', 'popular', 'becoming', 'start', 'Beyonce', 'did', 'When']\n",
      "Y_words: ['1990s', 'late', 'the', 'in', '?', 'popular', 'becoming', 'start', 'Beyonce', 'did', '<qw>']\n",
      "\n",
      "X_words: ['dancing', 'and', 'singing', '?', 'up', 'growing', 'was', 'she', 'when', 'in', 'compete', 'Beyonce', 'did', 'areas', 'What']\n",
      "Y_words: ['dancing', 'and', 'singing', '?', 'up', 'growing', 'was', 'she', 'when', 'in', 'compete', 'Beyonce', 'did', 'areas', '<qw>']\n",
      "\n",
      "X_words: ['2003', '?', 'singer', 'solo', 'a', 'become', 'and', 'Child', \"'s\", 'Destiny', 'leave', 'Beyonce', 'did', 'When']\n",
      "Y_words: ['2003', '?', 'singer', 'solo', 'a', 'become', 'and', 'Child', \"'s\", 'Destiny', 'leave', 'Beyonce', 'did', '<qw>']\n",
      "\n",
      "X_words: ['Texas', ',', 'Houston', '?', 'up', 'grow', 'Beyonce', 'did', 'state', 'and', 'city', 'what', 'In']\n",
      "Y_words: ['Texas', ',', 'Houston', '?', 'up', 'grow', 'Beyonce', 'did', 'state', 'and', 'city', 'what', '<qw>']\n",
      "\n",
      "X_words: ['1990s', 'late', '?', 'famous', 'become', 'Beyonce', 'did', 'decade', 'which', 'In']\n",
      "Y_words: ['1990s', 'late', '?', 'famous', 'become', 'Beyonce', 'did', 'decade', 'which', '<qw>']\n"
     ]
    }
   ],
   "source": [
    "x_words = [merged_question[::-1] for merged_question in merged]\n",
    "y_words = [hidden_answer[::-1] for hidden_answer in hidden_merged]\n",
    "\n",
    "for i in range(5):\n",
    "    print()\n",
    "    print(\"X_words:\", x_words[i])\n",
    "    print(\"Y_words:\", y_words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X: [21544, 48170, 49668, 31874, 31149, 3602, 14896, 1835, 21150, 12618, 65083]\n",
      "Y: [21544, 48170, 49668, 31874, 31149, 3602, 14896, 1835, 21150, 12618, 67298]\n",
      "\n",
      "X: [20898, 61937, 46659, 31149, 41438, 30710, 15960, 33499, 30236, 31874, 45462, 21150, 12618, 29146, 25588]\n",
      "Y: [20898, 61937, 46659, 31149, 41438, 30710, 15960, 33499, 30236, 31874, 45462, 21150, 12618, 29146, 67298]\n",
      "\n",
      "X: [23869, 31149, 41559, 66569, 59427, 4070, 61937, 16401, 45391, 41880, 61146, 21150, 12618, 65083]\n",
      "Y: [23869, 31149, 41559, 66569, 59427, 4070, 61937, 16401, 45391, 41880, 61146, 21150, 12618, 67298]\n",
      "\n",
      "X: [17821, 13572, 3213, 31149, 41438, 16864, 21150, 12618, 22201, 61937, 43286, 43050, 49514]\n",
      "Y: [17821, 13572, 3213, 31149, 41438, 16864, 21150, 12618, 22201, 61937, 43286, 43050, 67298]\n",
      "\n",
      "X: [21544, 48170, 31149, 31935, 4070, 21150, 12618, 1549, 66352, 49514]\n",
      "Y: [21544, 48170, 31149, 31935, 4070, 21150, 12618, 1549, 66352, 67298]\n"
     ]
    }
   ],
   "source": [
    "# Convert words to indices\n",
    "def words_to_indices(words):\n",
    "    return [word_to_idx[word] for word in words]\n",
    "\n",
    "\n",
    "x = [words_to_indices(words) for words in x_words]\n",
    "y = [words_to_indices(words) for words in y_words]\n",
    "\n",
    "for i in range(5):\n",
    "    print()\n",
    "    print(\"X:\", x[i])\n",
    "    print(\"Y:\", y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X: [21544, 48170, 49668, 31874, 31149, 3602, 14896, 1835, 21150, 12618, 65083, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299]\n",
      "Y: [21544, 48170, 49668, 31874, 31149, 3602, 14896, 1835, 21150, 12618, 67298, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299]\n",
      "\n",
      "X: [20898, 61937, 46659, 31149, 41438, 30710, 15960, 33499, 30236, 31874, 45462, 21150, 12618, 29146, 25588, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299]\n",
      "Y: [20898, 61937, 46659, 31149, 41438, 30710, 15960, 33499, 30236, 31874, 45462, 21150, 12618, 29146, 67298, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299]\n",
      "\n",
      "X: [23869, 31149, 41559, 66569, 59427, 4070, 61937, 16401, 45391, 41880, 61146, 21150, 12618, 65083, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299]\n",
      "Y: [23869, 31149, 41559, 66569, 59427, 4070, 61937, 16401, 45391, 41880, 61146, 21150, 12618, 67298, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299]\n",
      "\n",
      "X: [17821, 13572, 3213, 31149, 41438, 16864, 21150, 12618, 22201, 61937, 43286, 43050, 49514, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299]\n",
      "Y: [17821, 13572, 3213, 31149, 41438, 16864, 21150, 12618, 22201, 61937, 43286, 43050, 67298, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299]\n",
      "\n",
      "X: [21544, 48170, 31149, 31935, 4070, 21150, 12618, 1549, 66352, 49514, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299]\n",
      "Y: [21544, 48170, 31149, 31935, 4070, 21150, 12618, 1549, 66352, 67298, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299]\n"
     ]
    }
   ],
   "source": [
    "# Pad sequences\n",
    "max_len = max([len(words) for words in x])\n",
    "\n",
    "for i in range(len(x)):\n",
    "    x[i] += [word_to_idx[\"<pad>\"]] * (max_len - len(x[i]))\n",
    "    y[i] += [word_to_idx[\"<pad>\"]] * (max_len - len(y[i]))\n",
    "\n",
    "for i in range(5):\n",
    "    print()\n",
    "    print(\"X:\", x[i])\n",
    "    print(\"Y:\", y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensors\n",
    "x = torch.tensor(x).to(device)\n",
    "y = torch.tensor(y).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: ['1990s', 'late', 'the', 'in', '?', 'popular', 'becoming', 'start', 'Beyonce', 'did', 'When', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "Y: ['1990s', 'late', 'the', 'in', '?', 'popular', 'becoming', 'start', 'Beyonce', 'did', '<qw>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "# Example of a training pair (x, y) words\n",
    "i = 0\n",
    "print(\"X:\", [idx_to_word[idx.item()] for idx in x[i]])\n",
    "print(\"Y:\", [idx_to_word[idx.item()] for idx in y[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We will now our model to predict the first or the two first words of the question given the rest of the question and the answer. We will use a transformer model to do so.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 69456\n",
      "Val size: 8682\n",
      "Test size: 8683\n"
     ]
    }
   ],
   "source": [
    "# Train val test split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_val, y_val, test_size=0.5)\n",
    "\n",
    "print(\"Train size:\", len(x_train))\n",
    "print(\"Val size:\", len(x_val))\n",
    "print(\"Test size:\", len(x_test))\n",
    "\n",
    "# Create dataset\n",
    "\n",
    "\n",
    "class QuestionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "\n",
    "train_dataset = QuestionDataset(x_train, y_train)\n",
    "val_dataset = QuestionDataset(x_val, y_val)\n",
    "test_dataset = QuestionDataset(x_test, y_test)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM simple test model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94a0546009ec44fa8f9730434c9bf3f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca2f800445e94b6e82dcff07779ca63e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.9235, Val Loss: 0.3803\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "726fa57948174b38a2c1b399af26061c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Train Loss: 0.2385, Val Loss: 0.1969\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e4a2651054040f3919237e76a5c45c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Train Loss: 0.0996, Val Loss: 0.1361\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4a45f7d2efb4c30b30d39e8113e6cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Train Loss: 0.0379, Val Loss: 0.1178\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6631bc4d7fd748fd9327f7a303ba1d32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Train Loss: 0.0121, Val Loss: 0.1139\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "674bfe5a0b2848d4a1e53eb3e18d6dfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Train Loss: 0.0040, Val Loss: 0.1138\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c06b6cdcb6f4e378d96f08618370498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Train Loss: 0.0020, Val Loss: 0.1130\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccf405439eee4801b72c853822dabc94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Train Loss: 0.0014, Val Loss: 0.1114\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a972f27a34642259551dc0b0e729e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Train Loss: 0.0010, Val Loss: 0.1103\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e59ff91ab1144db8c6491d43e40eb1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Train Loss: 0.0007, Val Loss: 0.1099\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "class QuestionWordPredictor(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(QuestionWordPredictor, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 100\n",
    "\n",
    "model = QuestionWordPredictor(vocab_size, embedding_dim, hidden_dim).to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train model\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in trange(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for x_batch, y_batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x_batch)\n",
    "        loss = criterion(y_pred.view(-1, vocab_size), y_batch.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in val_loader:\n",
    "            y_pred = model(x_batch)\n",
    "            loss = criterion(y_pred.view(-1, vocab_size), y_batch.view(-1))\n",
    "            val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {\n",
    "          epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1114\n",
      "Train Accuracy: 0.9999\n",
      "Val Accuracy: 0.9925\n",
      "Test Accuracy: 0.9923\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        y_pred = model(x_batch)\n",
    "        loss = criterion(y_pred.view(-1, vocab_size), y_batch.view(-1))\n",
    "        test_loss += loss.item()\n",
    "    test_loss /= len(test_loader)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "def accuracy(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in loader:\n",
    "            y_pred = model(x_batch)\n",
    "            y_pred = y_pred.argmax(dim=-1)\n",
    "            correct += (y_pred == y_batch).sum().item()\n",
    "            total += y_batch.numel()\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "train_accuracy = accuracy(model, train_loader)\n",
    "val_accuracy = accuracy(model, val_loader)\n",
    "test_accuracy = accuracy(model, test_loader)\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Val Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.exists(\"models\"):\n",
    "    os.mkdir(\"models\")\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), \"models/question_word_predictor.pth\")\n",
    "\n",
    "# Load model\n",
    "model = QuestionWordPredictor(vocab_size, embedding_dim, hidden_dim).to(device)\n",
    "model.load_state_dict(torch.load(\"models/question_word_predictor.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: ['1963', '?', 'die', 'XXIII', 'John', 'Pope', 'did', 'year', 'what', 'In', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "Y: ['1963', '?', 'die', 'XXIII', 'John', 'Pope', 'did', 'year', 'what', '<qw>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "Y_pred: ['1963', '?', 'die', 'XXIII', 'John', 'Pope', 'did', 'year', 'what', '<qw>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "def predict(model, x):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(x)\n",
    "        y_pred = y_pred.argmax(dim=-1)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "# Example of a training pair (x, y) words\n",
    "i = 0\n",
    "x_i = x_train[i].unsqueeze(0)\n",
    "y_pred = predict(model, x_i)\n",
    "print(\"X:\", [idx_to_word[idx.item()] for idx in x_i[0]])\n",
    "print(\"Y:\", [idx_to_word[idx.item()] for idx in y_train[i]])\n",
    "print(\"Y_pred:\", [idx_to_word[idx.item()] for idx in y_pred[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
