{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question word prediction\n",
    "\n",
    "> Group 12: Tristan Perrot & Romain Darous\n",
    "\n",
    "Task is to train and evaluate a QWP model using any available QA-corpus, for instance, the [SQuAD corpus](https://rajpurkar.github.io/SQuAD-explorer/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA H100 80GB HBM3 MIG 1g.10gb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(torch.cuda.device_count()):\n",
    "    print(torch.cuda.get_device_properties(i).name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions: 86821\n",
      "\n",
      "Question: When did Beyonce start becoming popular?\n",
      "Answer: in the late 1990s\n",
      "\n",
      "Question: What areas did Beyonce compete in when she was growing up?\n",
      "Answer: singing and dancing\n",
      "\n",
      "Question: When did Beyonce leave Destiny's Child and become a solo singer?\n",
      "Answer: 2003\n",
      "\n",
      "Question: In what city and state did Beyonce  grow up? \n",
      "Answer: Houston, Texas\n",
      "\n",
      "Question: In which decade did Beyonce become famous?\n",
      "Answer: late 1990s\n"
     ]
    }
   ],
   "source": [
    "if data_dir not in os.listdir():\n",
    "    os.mkdir(data_dir)\n",
    "\n",
    "if \"squad_train.json\" not in os.listdir(data_dir):\n",
    "    # Download data at https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
    "    res = requests.get(\"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\")\n",
    "    data = json.loads(res.text)\n",
    "\n",
    "    # Save data to file\n",
    "    with open(data_dir + \"/squad_train.json\", \"w\") as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "with open(data_dir + \"/squad_train.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract answer text and question text\n",
    "answers = []\n",
    "questions = []\n",
    "for article in data[\"data\"]:\n",
    "    for paragraph in article[\"paragraphs\"]:\n",
    "        for qa in paragraph[\"qas\"]:\n",
    "            if qa[\"is_impossible\"]:\n",
    "                continue\n",
    "            answers.append(qa[\"answers\"][0][\"text\"])\n",
    "            questions.append(qa[\"question\"])\n",
    "\n",
    "print(\"Number of questions:\", len(questions))\n",
    "\n",
    "# Print some examples\n",
    "for i in range(5):\n",
    "    print()\n",
    "    print(\"Question:\", questions[i])\n",
    "    print(\"Answer:\", answers[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: ['When', 'did', 'Beyonce', 'start', 'becoming', 'popular', '?']\n",
      "Answer: ['in', 'the', 'late', '1990s']\n",
      "\n",
      "Question: ['What', 'areas', 'did', 'Beyonce', 'compete', 'in', 'when', 'she', 'was', 'growing', 'up', '?']\n",
      "Answer: ['singing', 'and', 'dancing']\n",
      "\n",
      "Question: ['When', 'did', 'Beyonce', 'leave', 'Destiny', \"'s\", 'Child', 'and', 'become', 'a', 'solo', 'singer', '?']\n",
      "Answer: ['2003']\n",
      "\n",
      "Question: ['In', 'what', 'city', 'and', 'state', 'did', 'Beyonce', 'grow', 'up', '?']\n",
      "Answer: ['Houston', ',', 'Texas']\n",
      "\n",
      "Question: ['In', 'which', 'decade', 'did', 'Beyonce', 'become', 'famous', '?']\n",
      "Answer: ['late', '1990s']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize questions\n",
    "tokenized_questions = [nltk.word_tokenize(q) for q in questions]\n",
    "\n",
    "# Tokenize answers\n",
    "tokenized_answers = [nltk.word_tokenize(a) for a in answers]\n",
    "\n",
    "for i in range(5):\n",
    "    print()\n",
    "    print(\"Question:\", tokenized_questions[i])\n",
    "    print(\"Answer:\", tokenized_answers[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merged: ['When', 'did', 'Beyonce', 'start', 'becoming', 'popular', '?', 'in', 'the', 'late', '1990s']\n",
      "\n",
      "Merged: ['What', 'areas', 'did', 'Beyonce', 'compete', 'in', 'when', 'she', 'was', 'growing', 'up', '?', 'singing', 'and', 'dancing']\n",
      "\n",
      "Merged: ['When', 'did', 'Beyonce', 'leave', 'Destiny', \"'s\", 'Child', 'and', 'become', 'a', 'solo', 'singer', '?', '2003']\n",
      "\n",
      "Merged: ['In', 'what', 'city', 'and', 'state', 'did', 'Beyonce', 'grow', 'up', '?', 'Houston', ',', 'Texas']\n",
      "\n",
      "Merged: ['In', 'which', 'decade', 'did', 'Beyonce', 'become', 'famous', '?', 'late', '1990s']\n"
     ]
    }
   ],
   "source": [
    "# Merge questions and answers\n",
    "merged = [q + a for q, a in zip(tokenized_questions, tokenized_answers)]\n",
    "\n",
    "for i in range(5):\n",
    "    print()\n",
    "    print(\"Merged:\", merged[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary\n",
    "vocab = set()\n",
    "for m in merged:\n",
    "    vocab.update(m)\n",
    "\n",
    "vocab = list(vocab)\n",
    "\n",
    "# Add \"<qw>\" to vocabulary\n",
    "vocab.append(\"<qw>\")\n",
    "vocab.append(\"<pad>\")\n",
    "\n",
    "# Create word to index and index to word mappings\n",
    "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "idx_to_word = {i: word for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merged: ['When', 'did', 'Beyonce', 'start', 'becoming', 'popular', '?', 'in', 'the', 'late', '1990s']\n",
      "Hidden: ['<qw>', 'did', 'Beyonce', 'start', 'becoming', 'popular', '?', 'in', 'the', 'late', '1990s']\n",
      "\n",
      "Merged: ['What', 'areas', 'did', 'Beyonce', 'compete', 'in', 'when', 'she', 'was', 'growing', 'up', '?', 'singing', 'and', 'dancing']\n",
      "Hidden: ['<qw>', 'areas', 'did', 'Beyonce', 'compete', 'in', 'when', 'she', 'was', 'growing', 'up', '?', 'singing', 'and', 'dancing']\n",
      "\n",
      "Merged: ['When', 'did', 'Beyonce', 'leave', 'Destiny', \"'s\", 'Child', 'and', 'become', 'a', 'solo', 'singer', '?', '2003']\n",
      "Hidden: ['<qw>', 'did', 'Beyonce', 'leave', 'Destiny', \"'s\", 'Child', 'and', 'become', 'a', 'solo', 'singer', '?', '2003']\n",
      "\n",
      "Merged: ['In', 'what', 'city', 'and', 'state', 'did', 'Beyonce', 'grow', 'up', '?', 'Houston', ',', 'Texas']\n",
      "Hidden: ['<qw>', 'what', 'city', 'and', 'state', 'did', 'Beyonce', 'grow', 'up', '?', 'Houston', ',', 'Texas']\n",
      "\n",
      "Merged: ['In', 'which', 'decade', 'did', 'Beyonce', 'become', 'famous', '?', 'late', '1990s']\n",
      "Hidden: ['<qw>', 'which', 'decade', 'did', 'Beyonce', 'become', 'famous', '?', 'late', '1990s']\n"
     ]
    }
   ],
   "source": [
    "words_to_hide = 1\n",
    "\n",
    "hidden_merged = []\n",
    "\n",
    "for m in merged:\n",
    "    hidden = m.copy()\n",
    "    hidden[:words_to_hide] = [\"<qw>\"] * words_to_hide\n",
    "    hidden_merged.append(hidden)\n",
    "\n",
    "for i in range(5):\n",
    "    print()\n",
    "    print(\"Merged:\", merged[i])\n",
    "    print(\"Hidden:\", hidden_merged[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_words: ['1990s', 'late', 'the', 'in', '?', 'popular', 'becoming', 'start', 'Beyonce', 'did', 'When']\n",
      "Y_words: ['1990s', 'late', 'the', 'in', '?', 'popular', 'becoming', 'start', 'Beyonce', 'did', '<qw>']\n",
      "\n",
      "X_words: ['dancing', 'and', 'singing', '?', 'up', 'growing', 'was', 'she', 'when', 'in', 'compete', 'Beyonce', 'did', 'areas', 'What']\n",
      "Y_words: ['dancing', 'and', 'singing', '?', 'up', 'growing', 'was', 'she', 'when', 'in', 'compete', 'Beyonce', 'did', 'areas', '<qw>']\n",
      "\n",
      "X_words: ['2003', '?', 'singer', 'solo', 'a', 'become', 'and', 'Child', \"'s\", 'Destiny', 'leave', 'Beyonce', 'did', 'When']\n",
      "Y_words: ['2003', '?', 'singer', 'solo', 'a', 'become', 'and', 'Child', \"'s\", 'Destiny', 'leave', 'Beyonce', 'did', '<qw>']\n",
      "\n",
      "X_words: ['Texas', ',', 'Houston', '?', 'up', 'grow', 'Beyonce', 'did', 'state', 'and', 'city', 'what', 'In']\n",
      "Y_words: ['Texas', ',', 'Houston', '?', 'up', 'grow', 'Beyonce', 'did', 'state', 'and', 'city', 'what', '<qw>']\n",
      "\n",
      "X_words: ['1990s', 'late', '?', 'famous', 'become', 'Beyonce', 'did', 'decade', 'which', 'In']\n",
      "Y_words: ['1990s', 'late', '?', 'famous', 'become', 'Beyonce', 'did', 'decade', 'which', '<qw>']\n"
     ]
    }
   ],
   "source": [
    "x_words = [merged_question[::-1] for merged_question in merged]\n",
    "y_words = [hidden_answer[::-1] for hidden_answer in hidden_merged]\n",
    "\n",
    "for i in range(5):\n",
    "    print()\n",
    "    print(\"X_words:\", x_words[i])\n",
    "    print(\"Y_words:\", y_words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X: [24509, 21704, 6747, 17845, 19785, 35594, 21598, 32091, 31579, 27604, 16793]\n",
      "Y: [24509, 21704, 6747, 17845, 19785, 35594, 21598, 32091, 31579, 27604, 67298]\n",
      "\n",
      "X: [43680, 40745, 29391, 19785, 19222, 17204, 19904, 4439, 42787, 17845, 9036, 31579, 27604, 63737, 57645]\n",
      "Y: [43680, 40745, 29391, 19785, 19222, 17204, 19904, 4439, 42787, 17845, 9036, 31579, 27604, 63737, 67298]\n",
      "\n",
      "X: [66589, 19785, 2747, 39418, 12937, 15908, 40745, 4125, 38556, 9691, 42076, 31579, 27604, 16793]\n",
      "Y: [66589, 19785, 2747, 39418, 12937, 15908, 40745, 4125, 38556, 9691, 42076, 31579, 27604, 67298]\n",
      "\n",
      "X: [22369, 13553, 63701, 19785, 19222, 44201, 31579, 27604, 321, 40745, 50219, 22009, 22018]\n",
      "Y: [22369, 13553, 63701, 19785, 19222, 44201, 31579, 27604, 321, 40745, 50219, 22009, 67298]\n",
      "\n",
      "X: [24509, 21704, 19785, 7842, 15908, 31579, 27604, 45616, 37395, 22018]\n",
      "Y: [24509, 21704, 19785, 7842, 15908, 31579, 27604, 45616, 37395, 67298]\n"
     ]
    }
   ],
   "source": [
    "# Convert words to indices\n",
    "def words_to_indices(words):\n",
    "    return [word_to_idx[word] for word in words]\n",
    "\n",
    "x = [words_to_indices(words) for words in x_words]\n",
    "y = [words_to_indices(words) for words in y_words]\n",
    "\n",
    "for i in range(5):\n",
    "    print()\n",
    "    print(\"X:\", x[i])\n",
    "    print(\"Y:\", y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X: [24509, 21704, 6747, 17845, 19785, 35594, 21598, 32091, 31579, 27604, 16793, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299]\n",
      "Y: [24509, 21704, 6747, 17845, 19785, 35594, 21598, 32091, 31579, 27604, 67298, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299]\n",
      "\n",
      "X: [43680, 40745, 29391, 19785, 19222, 17204, 19904, 4439, 42787, 17845, 9036, 31579, 27604, 63737, 57645, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299]\n",
      "Y: [43680, 40745, 29391, 19785, 19222, 17204, 19904, 4439, 42787, 17845, 9036, 31579, 27604, 63737, 67298, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299]\n",
      "\n",
      "X: [66589, 19785, 2747, 39418, 12937, 15908, 40745, 4125, 38556, 9691, 42076, 31579, 27604, 16793, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299]\n",
      "Y: [66589, 19785, 2747, 39418, 12937, 15908, 40745, 4125, 38556, 9691, 42076, 31579, 27604, 67298, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299]\n",
      "\n",
      "X: [22369, 13553, 63701, 19785, 19222, 44201, 31579, 27604, 321, 40745, 50219, 22009, 22018, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299]\n",
      "Y: [22369, 13553, 63701, 19785, 19222, 44201, 31579, 27604, 321, 40745, 50219, 22009, 67298, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299]\n",
      "\n",
      "X: [24509, 21704, 19785, 7842, 15908, 31579, 27604, 45616, 37395, 22018, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299]\n",
      "Y: [24509, 21704, 19785, 7842, 15908, 31579, 27604, 45616, 37395, 67298, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299, 67299]\n"
     ]
    }
   ],
   "source": [
    "# Pad sequences\n",
    "max_len = max([len(words) for words in x])\n",
    "\n",
    "for i in range(len(x)):\n",
    "    x[i] += [word_to_idx[\"<pad>\"]] * (max_len - len(x[i]))\n",
    "    y[i] += [word_to_idx[\"<pad>\"]] * (max_len - len(y[i]))\n",
    "\n",
    "for i in range(5):\n",
    "    print()\n",
    "    print(\"X:\", x[i])\n",
    "    print(\"Y:\", y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensors\n",
    "x = torch.tensor(x).to(device)\n",
    "y = torch.tensor(y).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: ['1990s', 'late', 'the', 'in', '?', 'popular', 'becoming', 'start', 'Beyonce', 'did', 'When', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "Y: ['1990s', 'late', 'the', 'in', '?', 'popular', 'becoming', 'start', 'Beyonce', 'did', '<qw>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "# Example of a training pair (x, y) words\n",
    "i = 0\n",
    "print(\"X:\", [idx_to_word[idx.item()] for idx in x[i]])\n",
    "print(\"Y:\", [idx_to_word[idx.item()] for idx in y[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We will now our model to predict the first or the two first words of the question given the rest of the question and the answer. We will use a transformer model to do so.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 69456\n",
      "Val size: 8682\n",
      "Test size: 8683\n"
     ]
    }
   ],
   "source": [
    "# Train val test split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_val, y_val, test_size=0.5)\n",
    "\n",
    "print(\"Train size:\", len(x_train))\n",
    "print(\"Val size:\", len(x_val))\n",
    "print(\"Test size:\", len(x_test))\n",
    "\n",
    "# Create dataset\n",
    "class QuestionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "\n",
    "train_dataset = QuestionDataset(x_train, y_train)\n",
    "val_dataset = QuestionDataset(x_val, y_val)\n",
    "test_dataset = QuestionDataset(x_test, y_test)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM simple test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed892e1f62bb4f73acc1d1e73c38fd0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d4f8b0e9a1c48d08c26ef1bc29b03d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     39\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 40\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m train_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[1;32m     43\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "class QuestionWordPredictor(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(QuestionWordPredictor, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "# Initialize model\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 100\n",
    "\n",
    "model = QuestionWordPredictor(vocab_size, embedding_dim, hidden_dim).to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train model\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in trange(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for x_batch, y_batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x_batch)\n",
    "        loss = criterion(y_pred.view(-1, vocab_size), y_batch.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in val_loader:\n",
    "            y_pred = model(x_batch)\n",
    "            loss = criterion(y_pred.view(-1, vocab_size), y_batch.view(-1))\n",
    "            val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate model\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        y_pred = model(x_batch)\n",
    "        loss = criterion(y_pred.view(-1, vocab_size), y_batch.view(-1))\n",
    "        test_loss += loss.item()\n",
    "    test_loss /= len(test_loader)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Accuracy\n",
    "def accuracy(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in loader:\n",
    "            y_pred = model(x_batch)\n",
    "            y_pred = y_pred.argmax(dim=-1)\n",
    "            correct += (y_pred == y_batch).sum().item()\n",
    "            total += y_batch.numel()\n",
    "    return correct / total\n",
    "\n",
    "train_accuracy = accuracy(model, train_loader)\n",
    "val_accuracy = accuracy(model, val_loader)\n",
    "test_accuracy = accuracy(model, test_loader)\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Val Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"models\"):\n",
    "    os.mkdir(\"models\")\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), \"models/question_word_predictor.pth\")\n",
    "\n",
    "# Load model\n",
    "model = QuestionWordPredictor(vocab_size, embedding_dim, hidden_dim).to(device)\n",
    "model.load_state_dict(torch.load(\"models/question_word_predictor.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predict\n",
    "def predict(model, x):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(x)\n",
    "        y_pred = y_pred.argmax(dim=-1)\n",
    "    return y_pred\n",
    "\n",
    "# Example of a training pair (x, y) words\n",
    "i = 0\n",
    "x_i = x_train[i].unsqueeze(0)\n",
    "y_pred = predict(model, x_i)\n",
    "print(\"X:\", [idx_to_word[idx] for idx in x_i[0]])\n",
    "print(\"Y:\", [idx_to_word[idx] for idx in y_train[i]])\n",
    "print(\"Y_pred:\", [idx_to_word[idx] for idx in y_pred[0]])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
