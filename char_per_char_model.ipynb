{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "202e5b05-663a-4be5-adaf-088482568f94",
   "metadata": {},
   "source": [
    "# Question word prediction\n",
    "\n",
    "> Group 12: Tristan Perrot & Romain Darous\n",
    "\n",
    "Task is to train and evaluate a **char per char Transformer model** model using any available QA-corpus, for instance, the [SQuAD corpus](https://rajpurkar.github.io/SQuAD-explorer/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36f3a0c",
   "metadata": {},
   "source": [
    "# 0. Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37285237",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "\n",
    "# Importing\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "359b715e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA H100 80GB HBM3 MIG 1g.10gb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(torch.cuda.device_count()):\n",
    "    print(torch.cuda.get_device_properties(i).name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available(\n",
    ") else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c009446",
   "metadata": {},
   "source": [
    "# 1. Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50678183",
   "metadata": {},
   "source": [
    "## 1.1. Loading the dataset\n",
    "**Note :** we only want to be able te recover the beginning of a question. For that, it doesn\"t matter whether the question is impossible to answer or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a59dd1d-d079-4907-95a6-f7a24b8aea24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions: 130319\n",
      "\n",
      "Question: When did Beyonce start becoming popular?\n",
      "Answer: in the late 1990s\n",
      "\n",
      "Question: What areas did Beyonce compete in when she was growing up?\n",
      "Answer: singing and dancing\n",
      "\n",
      "Question: When did Beyonce leave Destiny's Child and become a solo singer?\n",
      "Answer: 2003\n",
      "\n",
      "Question: In what city and state did Beyonce  grow up? \n",
      "Answer: Houston, Texas\n",
      "\n",
      "Question: In which decade did Beyonce become famous?\n",
      "Answer: late 1990s\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'data'\n",
    "if data_dir not in os.listdir():\n",
    "    os.mkdir(data_dir)\n",
    "\n",
    "if \"squad_train.json\" not in os.listdir(data_dir):\n",
    "    # Download data at https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
    "    res = requests.get(\n",
    "        \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\")\n",
    "    data = json.loads(res.text)\n",
    "\n",
    "    # Save data to file\n",
    "    with open(data_dir + \"/squad_train.json\", \"w\") as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "with open(data_dir + \"/squad_train.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract answer text and question text\n",
    "answers = []\n",
    "questions = []\n",
    "for article in data[\"data\"]:\n",
    "    for paragraph in article[\"paragraphs\"]:\n",
    "        for qa in paragraph[\"qas\"]:\n",
    "            questions.append(qa[\"question\"])\n",
    "            if qa[\"is_impossible\"]:\n",
    "                answers.append(\"\")\n",
    "            else :\n",
    "                answers.append(qa[\"answers\"][0][\"text\"])\n",
    "\n",
    "print(\"Number of questions:\", len(questions))\n",
    "\n",
    "# Print some examples\n",
    "for i in range(5):\n",
    "    print()\n",
    "    print(\"Question:\", questions[i])\n",
    "    print(\"Answer:\", answers[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d185c76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In which decade did Beyonce become famous? late 1990s\n"
     ]
    }
   ],
   "source": [
    "print(questions[i] + ' ' + answers[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94c8c00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing models\n",
    "import char_dataset\n",
    "import cpc_model\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1ddcfb",
   "metadata": {},
   "source": [
    "## 1.2. Making a suitable dataset\n",
    "``<BOS>`` token. Indicates that the sentence is starting.\n",
    "\n",
    "We will make the prediction of the sentence in reverse mode, as we want to predict the beginning of a question. We will use unidirectionnal attention as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2805192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating questions and answers\n",
    "dataset = [(questions[i].lower() + ' ' + answers[i].lower())[::-1] for i in range(len(questions))]\n",
    "# Splitting into train, validation, and test sets\n",
    "train_dataset, test_dataset = train_test_split(dataset, test_size = int(0.1*len(dataset)), train_size=int(0.9*len(dataset)))\n",
    "train_dataset, val_dataset = train_test_split(train_dataset, train_size=int(0.85*len(train_dataset)), test_size = int(0.15*len(train_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1fcf626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dataset : 130319\n",
      "Size of the train, val and test sets : (99693, 17593, 13031)\n",
      "Example of original datapoint : When did Beyonce start becoming popular? in the late 1990s\n",
      "Example of formatted datapoint : s0991 etal eht ni ?ralupop gnimoceb trats ecnoyeb did nehw\n"
     ]
    }
   ],
   "source": [
    "print(f\"Size of the dataset : {len(dataset)}\")\n",
    "print(f\"Size of the train, val and test sets : {len(train_dataset), len(val_dataset), len(test_dataset)}\")\n",
    "print(f\"Example of original datapoint : {questions[0] + ' ' + answers[0]}\")\n",
    "print(f\"Example of formatted datapoint : {dataset[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f52619",
   "metadata": {},
   "source": [
    "## 1.3. Building a character dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ae36b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAXLEN : 49\n"
     ]
    }
   ],
   "source": [
    "# Computing MAXLEN parameter. It's the min length among all the questions when removing the two first words\n",
    "# Will be used as the max window size for context\n",
    "MAXLEN = len(\" \".join(dataset[0].split(' ')[:-2]))\n",
    "\n",
    "for i in range(1, len(dataset)) :\n",
    "    tmp_len = len(\" \".join(dataset[0].split(' ')[:-2]))\n",
    "    if tmp_len < MAXLEN : MAXLEN = tmp_len\n",
    "\n",
    "print(f\"MAXLEN : {MAXLEN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f91fb4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating models# Delete the modules from the namespace\n",
    "del char_dataset\n",
    "del cpc_model\n",
    "\n",
    "# Unload the modules from memory\n",
    "import sys\n",
    "del sys.modules['char_dataset']\n",
    "del sys.modules['cpc_model']\n",
    "\n",
    "# Importing models\n",
    "import char_dataset\n",
    "import cpc_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f006917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the datasets\n",
    "train_char_set = char_dataset.CharDataset(train_dataset, MAXLEN)\n",
    "val_char_set = char_dataset.CharDataset(val_dataset, MAXLEN)\n",
    "test_char_set = char_dataset.CharDataset(test_dataset, MAXLEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1c2b1c",
   "metadata": {},
   "source": [
    "# 2. The model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58338cf8",
   "metadata": {},
   "source": [
    "## 2.2. Setting of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f42e1a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= Hyper-parameters for training ============== #\n",
    "\n",
    "class Config :\n",
    "    number_of_transformer_encoders = 1\n",
    "    number_of_attention_heads = 1\n",
    "    hidden_size = 64\n",
    "    dropout_prob = 0.1\n",
    "    batch_size = 64\n",
    "    learning_rate = 0.0003\n",
    "    weight_decay = 0.000001\n",
    "    no_of_epochs = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e7de04",
   "metadata": {},
   "source": [
    "## 2.1. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab7b047d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'PositionwiseFFN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m config \u001b[38;5;241m=\u001b[39m Config()\n\u001b[1;32m      7\u001b[0m training_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_char_set, batch_size\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[0;32m----> 9\u001b[0m charlm \u001b[38;5;241m=\u001b[39m \u001b[43mcpc_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCharLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchar_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCharDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid_to_char\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMAXLEN\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     11\u001b[0m charlm_optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam( charlm\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlearning_rate )\n",
      "File \u001b[0;32m~/DD2417-NLP-KTH-mini-project/cpc_model.py:135\u001b[0m, in \u001b[0;36mCharLM.__init__\u001b[0;34m(self, config, no_of_input_chars, MAXLEN)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mfill_(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMAXLEN, config\u001b[38;5;241m.\u001b[39mhidden_size))\n\u001b[0;32m--> 135\u001b[0m modules \u001b[38;5;241m=\u001b[39m [EncoderBlock(config\u001b[38;5;241m.\u001b[39mhidden_size, \n\u001b[1;32m    136\u001b[0m                         config\u001b[38;5;241m.\u001b[39mnumber_of_attention_heads,\n\u001b[1;32m    137\u001b[0m                         config\u001b[38;5;241m.\u001b[39mdropout_prob) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnumber_of_transformer_encoders)]\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(modules)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(config\u001b[38;5;241m.\u001b[39mhidden_size\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMAXLEN, no_of_input_chars)\n",
      "File \u001b[0;32m~/DD2417-NLP-KTH-mini-project/cpc_model.py:135\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mfill_(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMAXLEN, config\u001b[38;5;241m.\u001b[39mhidden_size))\n\u001b[0;32m--> 135\u001b[0m modules \u001b[38;5;241m=\u001b[39m [\u001b[43mEncoderBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumber_of_attention_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout_prob\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnumber_of_transformer_encoders)]\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(modules)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(config\u001b[38;5;241m.\u001b[39mhidden_size\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMAXLEN, no_of_input_chars)\n",
      "File \u001b[0;32m~/DD2417-NLP-KTH-mini-project/cpc_model.py:109\u001b[0m, in \u001b[0;36mEncoderBlock.__init__\u001b[0;34m(self, hidden_size, number_of_attention_heads, dropout_prob)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn \u001b[38;5;241m=\u001b[39m MultiHeadSelfAttention(hidden_size, number_of_attention_heads)\n\u001b[0;32m--> 109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn \u001b[38;5;241m=\u001b[39m \u001b[43mPositionwiseFFN\u001b[49m(hidden_size, dropout_prob)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(dropout_prob)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_size)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PositionwiseFFN' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# ======================= Training ======================= #\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print( \"Running on\", device )\n",
    "\n",
    "config = Config()\n",
    "training_loader = DataLoader(train_char_set, batch_size=config.batch_size)\n",
    "\n",
    "charlm = cpc_model.CharLM( config, len(char_dataset.CharDataset.id_to_char), MAXLEN).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "charlm_optimizer = optim.Adam( charlm.parameters(), lr=config.learning_rate )\n",
    "\n",
    "charlm.train()\n",
    "print( datetime.now().strftime(\"%X\"), \"Training starts\" )\n",
    "for epoch in range(config.no_of_epochs) :\n",
    "    iteration = 0\n",
    "    for input_tensor, label in training_loader :\n",
    "        input_tensor, label = input_tensor.to(device), label.to(device)\n",
    "        charlm_optimizer.zero_grad()\n",
    "        logits = charlm(input_tensor).to(device)\n",
    "        loss = criterion(logits.squeeze(1), label)\n",
    "        loss.backward()\n",
    "        charlm_optimizer.step()\n",
    "        iteration += 1\n",
    "\n",
    "    print( datetime.now().strftime(\"%X\"), \"End of epoch\", epoch+1, \", loss=\", loss.detach().item())\n",
    "    charlm.eval()\n",
    "    \"\"\"# Generate 50 characters starting from the input text\n",
    "    try :\n",
    "        char_list = list(\"he took out his wand and\"[-MAXLEN:])\n",
    "        for i in range(300) :\n",
    "            input_tensor = torch.tensor( [char_dataset.CharDataset.char_to_id[c] for c in char_list] + [char_dataset.CharDataset.char_to_id[PADDING_SYMBOL]]*(MAXLEN-len(char_list))).unsqueeze(0).to(device)\n",
    "            logits = charlm(input_tensor).squeeze().to(device)\n",
    "            _, new_character_tensor = logits.topk(1)\n",
    "            new_character = char_dataset.CharDataset.id_to_char[new_character_tensor.detach().item()]\n",
    "            print( new_character, end='' )\n",
    "            if len(char_list) == MAXLEN :\n",
    "                char_list.pop(0)\n",
    "            char_list.append( new_character )\n",
    "        print()\n",
    "    except KeyError :\n",
    "        continue\"\"\"\n",
    "    charlm.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09448cd7",
   "metadata": {},
   "source": [
    "## 2.2. User interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e750f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== User interaction ==================== #\n",
    "\n",
    "while True:\n",
    "    text = input(\"> \").strip()\n",
    "    if text == \"\" :\n",
    "        continue\n",
    "    char_list = list(text[-MAXLEN:])\n",
    "    \n",
    "    char_list = char_list[::-1]\n",
    "    pred_char = char_list[-1]\n",
    "    full_question = char_list\n",
    "\n",
    "    # Recovering the beginning of the question\n",
    "    try :\n",
    "        count = 0\n",
    "        MAX_COUNT = 50\n",
    "        while pred_char != char_dataset.CharDataset.BOQ and count < MAX_COUNT :\n",
    "            input_tensor = torch.tensor( [char_dataset.CharDataset.char_to_id[c] for c in char_list]).unsqueeze(0).to(device)\n",
    "            logits = charlm(input_tensor).squeeze().to(device)\n",
    "            _, new_character_tensor = logits.topk(1)\n",
    "            new_character = char_dataset.CharDataset.id_to_char[new_character_tensor.detach().item()]\n",
    "            # Uploading context\n",
    "            char_list = char_list[1:] + new_character\n",
    "            full_question.append(new_character)\n",
    "        full_question = \"\".join(full_question[::-1])\n",
    "        print(f\"Recovered question : {full_question}\")\n",
    "    except KeyError :\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
